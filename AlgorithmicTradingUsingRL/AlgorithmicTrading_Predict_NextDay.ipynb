{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26282f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28270f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\silve\\anaconda3\\lib\\site-packages (0.2.38)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (4.9.2)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (2022.7)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (2.3.10)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (3.17.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\silve\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance) (2023.7.22)\n",
      "Requirement already satisfied: ta in c:\\users\\silve\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ta) (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ta) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas->ta) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->ta) (1.16.0)\n",
      "Requirement already satisfied: stable.baselines3 in c:\\users\\silve\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stable.baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stable.baselines3) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stable.baselines3) (2.2.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stable.baselines3) (2.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stable.baselines3) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stable.baselines3) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable.baselines3) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable.baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch>=1.13->stable.baselines3) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch>=1.13->stable.baselines3) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch>=1.13->stable.baselines3) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch>=1.13->stable.baselines3) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch>=1.13->stable.baselines3) (2023.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from matplotlib->stable.baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas->stable.baselines3) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable.baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13->stable.baselines3) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13->stable.baselines3) (1.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\silve\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\silve\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\silve\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: cufflinks in c:\\users\\silve\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.19.2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (1.5.3)\n",
      "Requirement already satisfied: plotly>=4.1.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (5.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (1.16.0)\n",
      "Requirement already satisfied: colorlover>=0.2.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (0.3.0)\n",
      "Requirement already satisfied: setuptools>=34.4.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (68.0.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (8.12.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from cufflinks) (8.0.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.4.6)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->cufflinks) (6.19.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->cufflinks) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->cufflinks) (3.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->cufflinks) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas>=0.19.2->cufflinks) (2022.7)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from plotly>=4.1.1->cufflinks) (8.2.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (7.4.9)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (1.5.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (6.3.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->cufflinks) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\silve\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->cufflinks) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->cufflinks) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->cufflinks) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\silve\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->cufflinks) (0.2.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->cufflinks) (305.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\silve\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Based on the article:\n",
    "# https://medium.com/@mrconnor/harnessing-deep-reinforcement-learning-for-algorithmic-trading-d80c36ceda6e\n",
    "\n",
    "# Intall the required modules\n",
    "!pip install yfinance\n",
    "!pip install ta\n",
    "!pip install stable.baselines3\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install cufflinks\n",
    "!pip install -U gym\n",
    "#!pip install 'shimmy>=0.2.1'\n",
    "!pip install wandb -qU\n",
    "\n",
    "# Import the modules\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "#!pip install TA-Lib\n",
    "import warnings\n",
    "import yfinance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import ta\n",
    "import os\n",
    "#import talib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Optional Plotly Method Imports\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "from stable_baselines3 import A2C, PPO, DDPG\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy #MlpPolicy for A2C and PPO\n",
    "from stable_baselines3.td3.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import ActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e4416",
   "metadata": {},
   "source": [
    "# Parameters 1 - Stock and holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5836f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Yahoo Finance API - Stock Parameters ****\n",
    "\n",
    "# 14 stocks holdings (10 + schwab holdings)\n",
    "#tic = ['GOOGL', 'MSFT', 'AAPL', 'TSM', 'AMZN', 'META', 'NFLX', 'AMD', 'NVDA', 'CRM', 'PLTR', 'GOLD', 'SNOW', 'TSLA']\n",
    "\n",
    "#Schwab Holdings\n",
    "tic = ['AAPL','AMZN','GOLD','GOOG','GOOGL','META','MSFT','SMCI','SNOW','TSLA','TSM','CRM']\n",
    "Current_Tic_Holdings_list = [53,10,270,2,40,10,10,2,20,2,70,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa134f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of AAPL before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of AMZN before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of GOLD before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of GOOG before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of GOOGL before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of META before merging : (3028, 8) | after merging : (3879, 8)\n",
      "Shape of MSFT before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of SMCI before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of SNOW before merging : (933, 8) | after merging : (3879, 8)\n",
      "Shape of TSLA before merging : (3505, 8) | after merging : (3879, 8)\n",
      "Shape of TSM before merging : (3879, 8) | after merging : (3879, 8)\n",
      "Shape of CRM before merging : (3879, 8) | after merging : (3879, 8)\n"
     ]
    }
   ],
   "source": [
    "# Create start and end date for stock dataset\n",
    "todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "stock_start_date = '2009-01-01'\n",
    "#stock_end_date = todays_date\n",
    "\n",
    "# Create variables to hold info\n",
    "temp_data=[]\n",
    "stock_data=pd.DataFrame()\n",
    "\n",
    "# Download stock data from Yahoo Finance\n",
    "# and put into dataframe\n",
    "for s in tic:\n",
    "  temp_data = yfinance.download(s, start= stock_start_date)\n",
    "  temp_data['Ticker'] = s\n",
    "  stock_data = pd.concat([stock_data,temp_data])\n",
    "\n",
    "# Rename Adj Close\n",
    "stock_data = stock_data.rename(columns={'Adj Close':'AdjClose'})\n",
    "# Reset the index\n",
    "stock_data = stock_data.reset_index()\n",
    "# Sort the data by Date and Ticker and reset the index\n",
    "stock_data = stock_data.sort_values(by= ['Date', 'Ticker']).reset_index(drop = True)\n",
    "#fill NA values with 0\n",
    "stock_data = stock_data.fillna(0)\n",
    "\n",
    "# Find all unique dates to avoid duplicates\n",
    "uniq_date = stock_data.Date.unique()\n",
    "stocks = pd.DataFrame({\"Date\": uniq_date})\n",
    "\n",
    "# Create a copy of stock data and add technical indicators\n",
    "stock_data_tech = pd.DataFrame()\n",
    "\n",
    "# Make changes first calculate the technical indicators and then match all the stocks with dates and then backfill NaN with last observed valid data\n",
    "for i in tic:\n",
    "\n",
    "  # Force all the date ranges to be same for all stocks #DOW\n",
    "  temp = pd.merge(stocks, stock_data[stock_data.Ticker == i], how='left', on= 'Date')\n",
    "  # Fill the missing values\n",
    "  temp = temp.fillna(method='bfill')\n",
    "  print('Shape of {} before merging : {} | after merging : {}'.format(i, stock_data[stock_data.Ticker == i].shape, temp.shape))\n",
    "\n",
    "  # Add all the available technical indicators\n",
    "  stock_tech = ta.add_all_ta_features(df= temp, open= 'Open', high= 'High', low= 'Low', close='AdjClose', volume = 'Volume', fillna=True)\n",
    "  stock_data_tech = stock_data_tech._append(stock_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d42096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_rsi</th>\n",
       "      <th>trend_adx</th>\n",
       "      <th>trend_macd</th>\n",
       "      <th>trend_cci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>36.064999</td>\n",
       "      <td>37.116501</td>\n",
       "      <td>35.931999</td>\n",
       "      <td>37.108501</td>\n",
       "      <td>60112000.0</td>\n",
       "      <td>-0.359278</td>\n",
       "      <td>55.175378</td>\n",
       "      <td>15.314110</td>\n",
       "      <td>-0.130750</td>\n",
       "      <td>52.482264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>2019-10-14</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.149158</td>\n",
       "      <td>7369500.0</td>\n",
       "      <td>-0.958327</td>\n",
       "      <td>41.773701</td>\n",
       "      <td>12.751451</td>\n",
       "      <td>-0.150736</td>\n",
       "      <td>-121.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>23.497499</td>\n",
       "      <td>23.680000</td>\n",
       "      <td>23.127501</td>\n",
       "      <td>21.394478</td>\n",
       "      <td>274126000.0</td>\n",
       "      <td>-2.050796</td>\n",
       "      <td>21.571547</td>\n",
       "      <td>34.336060</td>\n",
       "      <td>-0.489120</td>\n",
       "      <td>-224.972150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>2011-09-19</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>14.178571</td>\n",
       "      <td>14.758214</td>\n",
       "      <td>14.114286</td>\n",
       "      <td>12.429061</td>\n",
       "      <td>823860800.0</td>\n",
       "      <td>1.655160</td>\n",
       "      <td>68.487172</td>\n",
       "      <td>13.032940</td>\n",
       "      <td>0.191822</td>\n",
       "      <td>196.582966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>META</td>\n",
       "      <td>488.170013</td>\n",
       "      <td>496.630005</td>\n",
       "      <td>481.279999</td>\n",
       "      <td>496.239990</td>\n",
       "      <td>10903100.0</td>\n",
       "      <td>2.662904</td>\n",
       "      <td>58.008836</td>\n",
       "      <td>41.065132</td>\n",
       "      <td>12.785635</td>\n",
       "      <td>-0.861833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>246.550003</td>\n",
       "      <td>246.589996</td>\n",
       "      <td>243.520004</td>\n",
       "      <td>238.445709</td>\n",
       "      <td>24970200.0</td>\n",
       "      <td>-0.509508</td>\n",
       "      <td>45.480587</td>\n",
       "      <td>25.970911</td>\n",
       "      <td>-1.233417</td>\n",
       "      <td>-85.065386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>2014-12-08</td>\n",
       "      <td>META</td>\n",
       "      <td>76.180000</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>75.400002</td>\n",
       "      <td>76.438896</td>\n",
       "      <td>25733900.0</td>\n",
       "      <td>0.121644</td>\n",
       "      <td>54.585985</td>\n",
       "      <td>15.320886</td>\n",
       "      <td>0.091789</td>\n",
       "      <td>106.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2010-10-06</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>8.030000</td>\n",
       "      <td>8.034000</td>\n",
       "      <td>7.730000</td>\n",
       "      <td>7.770000</td>\n",
       "      <td>121216000.0</td>\n",
       "      <td>4.179512</td>\n",
       "      <td>60.953946</td>\n",
       "      <td>40.384403</td>\n",
       "      <td>0.310939</td>\n",
       "      <td>61.227770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>CRM</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>82.589996</td>\n",
       "      <td>81.650002</td>\n",
       "      <td>82.112740</td>\n",
       "      <td>6217200.0</td>\n",
       "      <td>1.745831</td>\n",
       "      <td>61.474956</td>\n",
       "      <td>45.196477</td>\n",
       "      <td>1.403127</td>\n",
       "      <td>71.419925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>24.799999</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>262300.0</td>\n",
       "      <td>-1.010327</td>\n",
       "      <td>44.810282</td>\n",
       "      <td>18.247033</td>\n",
       "      <td>-0.252473</td>\n",
       "      <td>-69.320211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Ticker        Open        High         Low    AdjClose  \\\n",
       "1801 2016-03-01  GOOGL   36.064999   37.116501   35.931999   37.108501   \n",
       "2713 2019-10-14   GOLD   17.000000   17.299999   17.000000   15.149158   \n",
       "1843 2016-04-29   AAPL   23.497499   23.680000   23.127501   21.394478   \n",
       "683  2011-09-19   AAPL   14.178571   14.758214   14.114286   12.429061   \n",
       "3827 2024-03-19   META  488.170013  496.630005  481.279999  496.239990   \n",
       "3113 2021-05-17   MSFT  246.550003  246.589996  243.520004  238.445709   \n",
       "1493 2014-12-08   META   76.180000   77.250000   75.400002   76.438896   \n",
       "443  2010-10-06   AMZN    8.030000    8.034000    7.730000    7.770000   \n",
       "2055 2017-03-03    CRM   82.000000   82.589996   81.650002   82.112740   \n",
       "2089 2017-04-21   SMCI   24.700001   24.799999   24.450001   24.600000   \n",
       "\n",
       "           Volume  momentum_ppo  momentum_rsi  trend_adx  trend_macd  \\\n",
       "1801   60112000.0     -0.359278     55.175378  15.314110   -0.130750   \n",
       "2713    7369500.0     -0.958327     41.773701  12.751451   -0.150736   \n",
       "1843  274126000.0     -2.050796     21.571547  34.336060   -0.489120   \n",
       "683   823860800.0      1.655160     68.487172  13.032940    0.191822   \n",
       "3827   10903100.0      2.662904     58.008836  41.065132   12.785635   \n",
       "3113   24970200.0     -0.509508     45.480587  25.970911   -1.233417   \n",
       "1493   25733900.0      0.121644     54.585985  15.320886    0.091789   \n",
       "443   121216000.0      4.179512     60.953946  40.384403    0.310939   \n",
       "2055    6217200.0      1.745831     61.474956  45.196477    1.403127   \n",
       "2089     262300.0     -1.010327     44.810282  18.247033   -0.252473   \n",
       "\n",
       "       trend_cci  \n",
       "1801   52.482264  \n",
       "2713 -121.984200  \n",
       "1843 -224.972150  \n",
       "683   196.582966  \n",
       "3827   -0.861833  \n",
       "3113  -85.065386  \n",
       "1493  106.202482  \n",
       "443    61.227770  \n",
       "2055   71.419925  \n",
       "2089  -69.320211  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the stock data with wanted technical indicators\n",
    "stock_data_tech[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1bba60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_ppo_signal</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>momentum_pvo_signal</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>270.929993</td>\n",
       "      <td>274.959991</td>\n",
       "      <td>268.529999</td>\n",
       "      <td>272.290009</td>\n",
       "      <td>272.290009</td>\n",
       "      <td>8321600.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.195009e+09</td>\n",
       "      <td>979765900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305238</td>\n",
       "      <td>-0.438399</td>\n",
       "      <td>0.133160</td>\n",
       "      <td>1.968511</td>\n",
       "      <td>-7.461292</td>\n",
       "      <td>9.429803</td>\n",
       "      <td>281.246903</td>\n",
       "      <td>-2.254370</td>\n",
       "      <td>-2.280169</td>\n",
       "      <td>3105.710523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>273.100006</td>\n",
       "      <td>273.230011</td>\n",
       "      <td>267.160004</td>\n",
       "      <td>269.829987</td>\n",
       "      <td>269.829987</td>\n",
       "      <td>9294100.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.196127e+09</td>\n",
       "      <td>970471800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.572773</td>\n",
       "      <td>-0.465273</td>\n",
       "      <td>-0.107499</td>\n",
       "      <td>9.384442</td>\n",
       "      <td>-4.092145</td>\n",
       "      <td>13.476587</td>\n",
       "      <td>280.788268</td>\n",
       "      <td>-0.903457</td>\n",
       "      <td>-0.907562</td>\n",
       "      <td>3076.748320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>272.869995</td>\n",
       "      <td>267.769989</td>\n",
       "      <td>271.619995</td>\n",
       "      <td>271.619995</td>\n",
       "      <td>13397900.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.189297e+09</td>\n",
       "      <td>983869700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.725777</td>\n",
       "      <td>-0.517374</td>\n",
       "      <td>-0.208403</td>\n",
       "      <td>19.407071</td>\n",
       "      <td>0.607698</td>\n",
       "      <td>18.799372</td>\n",
       "      <td>280.572864</td>\n",
       "      <td>0.663384</td>\n",
       "      <td>0.661193</td>\n",
       "      <td>3097.822356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>223.399994</td>\n",
       "      <td>225.089996</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>218.009995</td>\n",
       "      <td>218.009995</td>\n",
       "      <td>66860900.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.194762e+09</td>\n",
       "      <td>917008800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.406149</td>\n",
       "      <td>-0.895129</td>\n",
       "      <td>-1.511020</td>\n",
       "      <td>57.728383</td>\n",
       "      <td>12.031835</td>\n",
       "      <td>45.696548</td>\n",
       "      <td>257.959282</td>\n",
       "      <td>-19.737133</td>\n",
       "      <td>-21.986310</td>\n",
       "      <td>2466.663893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>219.110001</td>\n",
       "      <td>234.619995</td>\n",
       "      <td>216.093002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>35984705.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.159477e+09</td>\n",
       "      <td>952993505.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.244339</td>\n",
       "      <td>-1.364971</td>\n",
       "      <td>-1.879368</td>\n",
       "      <td>58.297647</td>\n",
       "      <td>21.284997</td>\n",
       "      <td>37.012649</td>\n",
       "      <td>254.292464</td>\n",
       "      <td>7.536355</td>\n",
       "      <td>7.265879</td>\n",
       "      <td>2660.096806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close    AdjClose  \\\n",
       "3874 2024-05-24  270.929993  274.959991  268.529999  272.290009  272.290009   \n",
       "3875 2024-05-28  273.100006  273.230011  267.160004  269.829987  269.829987   \n",
       "3876 2024-05-29  268.000000  272.869995  267.769989  271.619995  271.619995   \n",
       "3877 2024-05-30  223.399994  225.089996  212.000000  218.009995  218.009995   \n",
       "3878 2024-05-31  219.110001  234.619995  216.093002  234.440002  234.440002   \n",
       "\n",
       "          Volume Ticker    volume_adi   volume_obv  ...  momentum_ppo  \\\n",
       "3874   8321600.0    CRM -1.195009e+09  979765900.0  ...     -0.305238   \n",
       "3875   9294100.0    CRM -1.196127e+09  970471800.0  ...     -0.572773   \n",
       "3876  13397900.0    CRM -1.189297e+09  983869700.0  ...     -0.725777   \n",
       "3877  66860900.0    CRM -1.194762e+09  917008800.0  ...     -2.406149   \n",
       "3878  35984705.0    CRM -1.159477e+09  952993505.0  ...     -3.244339   \n",
       "\n",
       "      momentum_ppo_signal  momentum_ppo_hist  momentum_pvo  \\\n",
       "3874            -0.438399           0.133160      1.968511   \n",
       "3875            -0.465273          -0.107499      9.384442   \n",
       "3876            -0.517374          -0.208403     19.407071   \n",
       "3877            -0.895129          -1.511020     57.728383   \n",
       "3878            -1.364971          -1.879368     58.297647   \n",
       "\n",
       "      momentum_pvo_signal  momentum_pvo_hist  momentum_kama  others_dr  \\\n",
       "3874            -7.461292           9.429803     281.246903  -2.254370   \n",
       "3875            -4.092145          13.476587     280.788268  -0.903457   \n",
       "3876             0.607698          18.799372     280.572864   0.663384   \n",
       "3877            12.031835          45.696548     257.959282 -19.737133   \n",
       "3878            21.284997          37.012649     254.292464   7.536355   \n",
       "\n",
       "      others_dlr    others_cr  \n",
       "3874   -2.280169  3105.710523  \n",
       "3875   -0.907562  3076.748320  \n",
       "3876    0.661193  3097.822356  \n",
       "3877  -21.986310  2466.663893  \n",
       "3878    7.265879  2660.096806  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_tech.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f046867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'AMZN', 'GOLD', 'GOOG', 'GOOGL', 'META', 'MSFT', 'SMCI', 'SNOW', 'TSLA', 'TSM', 'CRM']\n"
     ]
    }
   ],
   "source": [
    "# Display what stocks are in tic list\n",
    "print(tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced82ae",
   "metadata": {},
   "source": [
    "# Parameters 2 - Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Parameters   \n",
    "ticLen = len(tic)               # number of stocks in tic list\n",
    "Shares_Per_Trade = 9999         # Maximum shares that can be purchased or sold in a trade\n",
    "Initial_Investment = 20000      # Initial Investment\n",
    "Action_Space = ticLen           # Number of stocks\n",
    "\n",
    "# No. of Initial Investment + No. of Stocks Closing Prices + No. of Shares holding per stock + No. of stocks * No. of Technical Indicators\n",
    "\n",
    "No_Of_Initial_Investment = 1           # No. of Initial Investment\n",
    "No_Of_Closing_prices = Action_Space    # No. of Stocks Closing Prices\n",
    "#No_Of_Shares_Per_Stock = Action_Space  # No. of Shares held per stock\n",
    "\n",
    "No_Of_Shares_Per_Stock = len(Current_Tic_Holdings_list)\n",
    "\n",
    "No_Of_Stocks = Action_Space            # No. of stocks\n",
    "No_Of_Technical_Indicators = 5         # No. of Technical Indicators\n",
    "\n",
    "Observation_Space = No_Of_Initial_Investment + No_Of_Closing_prices + No_Of_Shares_Per_Stock + No_Of_Stocks * No_Of_Technical_Indicators\n",
    "\n",
    "Normalized_Rewards = 1e-2       # Factor used to normalized the rewards\n",
    "commission = 0                  # commission per trade\n",
    "\n",
    "run_optuma = 'no'              # optimize parameters using Optuma? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3491865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Stock Tracing Env 1 ****\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, Shares_Per_Trade = 10, Initial_Investment = 10000, Action_Space = ticLen, Observation_Space = 57, day = 0,\n",
    "                 Normalized_Rewards = 1e-4, verbosity = 0, mode = 'train', seed = 10, commission = 0  ):\n",
    "\n",
    "      self.day = day\n",
    "      self.df = df\n",
    "      self.max_shares_per_trade = Shares_Per_Trade\n",
    "      self.initial_investment = Initial_Investment\n",
    "      self.Action_Space = Action_Space\n",
    "      self.Observation_Space = Observation_Space\n",
    "      self.normalized_rewards = Normalized_Rewards\n",
    "      self.verbosity = verbosity\n",
    "      self.commission = commission\n",
    "      self.mode = mode\n",
    "      self._seed(seed)\n",
    "\n",
    "      #************************\n",
    "      self.ticker = df.Ticker.unique()\n",
    "\n",
    "\n",
    "      #Action Space\n",
    "      # Action > 0  means buy shares of stock\n",
    "      # Action 0 means Hold the stock\n",
    "      # Action < 0 means sell shares of stock\n",
    "      self.action_space = spaces.Box(low = -1, high= 1,\n",
    "                                    shape=(self.Action_Space,), dtype= int)\n",
    "      #Observation Space\n",
    "      self.observation_space = spaces.Box(low = -np.inf, high= np.inf, shape=(self.Observation_Space,))\n",
    "      #Selecting the Data for one date\n",
    "      self.data = self.df.loc[self.day,:]\n",
    "      #Initial Run\n",
    "      self.initial = True\n",
    "      #Verify if tradings days are completed or not\n",
    "      self.done = False\n",
    "      #Rewards\n",
    "      self.reward = 0\n",
    "      #Asset value after each trading day\n",
    "      self.asset_memory = [self.initial_investment]\n",
    "      #Rewards received for each trading day i.e profit or loss\n",
    "      self.reward_memory = []\n",
    "      #Saving the date for the trade\n",
    "      self.date_memory = [self.data.Date.unique()[0]]\n",
    "      #Initializing state of the environment\n",
    "      self.state = [self.initial_investment] + self.data.AdjClose.values.tolist() + Current_Tic_Holdings_list + \\\n",
    "        self.data.momentum_ppo.values.tolist() + self.data.momentum_rsi.values.tolist() + \\\n",
    "        self.data.trend_adx.values.tolist() + self.data.trend_macd.values.tolist() + \\\n",
    "        self.data.trend_cci.values.tolist()\n",
    "\n",
    "      #************************\n",
    "      self.keep_row_info = {}\n",
    "      self.keep_rows_list = []\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "      return self.state\n",
    "\n",
    "    # This method is used to reset the values of the state to it's default after every episode\n",
    "    def reset(self):\n",
    "      #print(\" \")\n",
    "      #print(\"reseting self...\")\n",
    "\n",
    "      self.day = 0\n",
    "      self.reward = 0\n",
    "      self.data = self.df.loc[self.day,:]\n",
    "      self.done = False\n",
    "      self.initial = False\n",
    "      self.reward_memory = []\n",
    "      self.date_memory = [str(self.data.Date.unique()[0])]\n",
    "      self.asset_memory = [self.initial_investment]\n",
    "      #***Technical Indicators tolist (check the  observation_space num !!!)\n",
    "      #***Search \"self.state =\"\n",
    "      self.state = [self.initial_investment] + self.data.AdjClose.values.tolist() + Current_Tic_Holdings_list + \\\n",
    "        self.data.momentum_ppo.values.tolist() + self.data.momentum_rsi.values.tolist() + \\\n",
    "        self.data.trend_adx.values.tolist() + self.data.trend_macd.values.tolist() + \\\n",
    "        self.data.trend_cci.values.tolist()\n",
    "    \n",
    "      #************************\n",
    "      self.keep_row_info = {}\n",
    "      self.keep_rows_list = []\n",
    "      #print(self.state)\n",
    "      return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "      #************************\n",
    "      #print(' ')\n",
    "      #print('************--start--************')\n",
    "\n",
    "\n",
    "\n",
    "      self.done = self.day >= len(self.df.Date.unique())-1\n",
    "      #Use this to save the results to csv after we performed trading for all the days\n",
    "      if self.done:\n",
    "        #@@@@@\n",
    "        keep_df = pd.DataFrame(index = [x for x in range(len(self.keep_rows_list))],data = self.keep_rows_list)\n",
    "\n",
    "        #print('Keep DF')\n",
    "        #print(keep_df)\n",
    "\n",
    "        final_portfolio_value = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "\n",
    "        total_rewards = final_portfolio_value - self.initial_investment\n",
    "        profit_pct = (total_rewards*100)/self.initial_investment\n",
    "        asset_df = pd.DataFrame(self.asset_memory)\n",
    "        asset_df.columns = ['portfolio']\n",
    "        asset_df['date'] = self.date_memory\n",
    "        #if self.verbosity and self.mode != 'train':\n",
    "        if self.verbosity :\n",
    "          #print(len(self.reward_memory))\n",
    "          #if self.mode == 'trade' or self.mode == 'val':\n",
    "\n",
    "          asset_df['date'] = self.date_memory\n",
    "\n",
    "\n",
    "          print( 'Initial Portfolio Value : {} | Final Portfolio Value : {} | Total rewards : {} | % of profit : {}'.format(self.initial_investment, final_portfolio_value,total_rewards, profit_pct))\n",
    "          asset_df.to_csv('{}_{}_results.csv'.format(self.mode, self.commission))\n",
    "\n",
    "\n",
    "\n",
    "        #************************\n",
    "        #keep_df = pd.DataFrame(keep_day_list, columns=['date'], index = [x for x in range(len(keep_day_list))])\n",
    "        #keep_df['portfolio_before_trade'] = keep_portfolio_before_trade_list\n",
    "        #keep_df['available_cash_after_trading'] = keep_available_cash_after_trading_list\n",
    "        #keep_df['shares_available_per_stock_list'] = keep_shares_available_per_stock_list\n",
    "        #keep_df['profit_and_loss_for_day'] = keep_profit_and_loss_for_day\n",
    "\n",
    "        #print(keep_df)\n",
    "\n",
    "        #keep_df.to_csv('keep_results.csv')\n",
    "\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "      else:\n",
    "\n",
    "        #Calculating the portfolio value before start of trading\n",
    "        #Available investment amount + sum of value of each stock held (no.of shares per stock * price of the stock on that day)\n",
    "        portfolio_before_trade = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "\n",
    "        #************************\n",
    "        keep_portfolio_before_trade = portfolio_before_trade\n",
    "        #keep_portfolio_before_trade_list.append(keep_portfolio_before_trade)\n",
    "        keep_available_cash_before_trading = self.state[0]\n",
    "\n",
    "        #Extracting the indicies of sell action\n",
    "        #if actions is not an array then convert it to array using np.array()\n",
    "        sell_indices = np.where(actions <  0 )[0]\n",
    "        #Extracting the indicies of buy action\n",
    "        buy_indices = np.where(actions >  0 )[0]\n",
    "\n",
    "\n",
    "        #************************\n",
    "        #Hold indicies\n",
    "        hold_indices = np.where(actions ==  0 )[0]\n",
    "\n",
    "\n",
    "\n",
    "        #print(actions)\n",
    "        #print(self.ticker)\n",
    "\n",
    "\n",
    "        ###### Trading starts #######\n",
    "        # Initially selling the stocks to increase investment value\n",
    "        for idx in sell_indices:\n",
    "\n",
    "          #************************\n",
    "          #print(\"Stock to sell: \" + str(idx))\n",
    "          sell_ticker_symbol = self.ticker[idx]\n",
    "          #print('Ticker: ' + str(sell_ticker_symbol))\n",
    "\n",
    "          stock_to_sell_ticker = 'Stock_to_sell_' + str(sell_ticker_symbol)\n",
    "          self.keep_row_info[stock_to_sell_ticker] = 'Yes'\n",
    "\n",
    "\n",
    "          #Sell stock if price is > 0 and shares held > 0\n",
    "          if self.state[idx+1] > 0 and self.state[idx+self.Action_Space+1] > 0:\n",
    "\n",
    "            #No of shares to sell\n",
    "            #No of shares to sell\n",
    "            #No of shares to sell\n",
    "            shares_sell = min(self.state[idx+self.Action_Space+1], abs(actions[idx]*self.max_shares_per_trade))\n",
    "            #shares_sell = int(min((self.state[idx+self.Action_Space+1]//1000)*1000, abs(actions[idx]*self.max_shares_per_trade))//1000)*1000\n",
    "\n",
    "            num_of_ticker_shares_available_before_selling = 'num_of_' + str(sell_ticker_symbol) + 'shares_available_before_selling'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_before_selling] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares to sell: ' + str(shares_sell))\n",
    "            num_of_ticker_to_sell = 'num_of_' + str(sell_ticker_symbol) + '_to_sell:'\n",
    "            self.keep_row_info[num_of_ticker_to_sell] = str(shares_sell)\n",
    "\n",
    "            #Updating the available cash after selling the stocks\n",
    "            self.state[0] += self.state[idx+1]*shares_sell*(1-self.commission)\n",
    "\n",
    "            #Updating the available stocks after selling\n",
    "            self.state[idx+self.Action_Space+1] -= shares_sell\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares available after selling: ' + str(self.state[idx+self.Action_Space+1]))\n",
    "            num_of_ticker_shares_available_after_selling = 'num_of_' + str(sell_ticker_symbol) + 'shares_available_after_selling'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_after_selling] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "\n",
    "            #stock_info_df = self.df\n",
    "            #wanted_df_1 = stock_info_df[stock_info_df['Date'] == date_string]\n",
    "            #wanted_df_2 = wanted_df_1[wanted_df_1['Ticker'] == sell_ticker_symbol]\n",
    "            #wanted_df_2 = wanted_df_2[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]]\n",
    "\n",
    "#@@@\n",
    "            #for w_column in wanted_df_2.columns.tolist():\n",
    "              #print(str(sell_ticker_symbol))\n",
    "              #print(str(w_column))\n",
    "              #print(str(wanted_df_2[w_column].values.tolist()[0]))\n",
    "              #print(str(sell_ticker_symbol) + '_' + str(w_column)+ ' : ' + str(wanted_df_2[w_column].values.tolist()))\n",
    "\n",
    "            #  self.keep_row_info[str(sell_ticker_symbol) + '_' + str(w_column)] = str(wanted_df_2[w_column].values.tolist()[0])\n",
    "              #pass\n",
    "\n",
    "\n",
    "\n",
    "          else:\n",
    "            # print('No Shares to sell')\n",
    "            pass\n",
    "        #print('Buying Stock shares : ')\n",
    "        for idx in buy_indices:\n",
    "\n",
    "          #************************\n",
    "          #print(\"Stock to buy: \" + str(idx))\n",
    "          buy_ticker_symbol = self.ticker[idx]\n",
    "          #print('Ticker: ' + str(buy_ticker_symbol))\n",
    "\n",
    "          stock_to_buy_ticker = 'Stock_to_buy_' + str(buy_ticker_symbol)\n",
    "          self.keep_row_info[stock_to_buy_ticker] = 'Yes'\n",
    "\n",
    "          #Buy stocks if price is > 0\n",
    "          if self.state[idx+1] > 0 and self.state[0] > 0:\n",
    "\n",
    "            #Max number of shares that can be brought with the available cash (available cash / stock price)\n",
    "            max_shares_buy = self.state[0]*(1 - self.commission)//self.state[idx+1]\n",
    "            #max_shares_buy = int(self.state[0]*(1 - self.commission)//self.state[idx+1]//1000)*1000\n",
    "\n",
    "            #No of shares to buy\n",
    "            shares_buy = min(max_shares_buy, actions[idx]*self.max_shares_per_trade)\n",
    "            #shares_buy = ((min(max_shares_buy, actions[idx]*self.max_shares_per_trade))//1000)*1000\n",
    "\n",
    "            num_of_ticker_shares_available_before_buying = 'num_of_' + str(buy_ticker_symbol) + 'shares_available_before_buying'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_before_buying] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares to buy: ' + str(shares_buy))\n",
    "            num_of_ticker_to_buy = 'num_of_' + str(buy_ticker_symbol) + '_to_buy:'\n",
    "            self.keep_row_info[num_of_ticker_to_buy] = str(shares_buy)\n",
    "\n",
    "            #Updating the available cash after buying the stocks\n",
    "            self.state[0] -= self.state[idx+1]*shares_buy*(1 + self.commission)\n",
    "\n",
    "            #Updating the available stocks after buying\n",
    "            self.state[idx+self.Action_Space+1] += shares_buy\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares available after buying: ' + str(self.state[idx+self.Action_Space+1]))\n",
    "            num_of_ticker_shares_available_after_buying = 'num_of_' + str(buy_ticker_symbol) + 'shares_available_after_buying'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_after_buying] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "            #stock_info_df = self.df\n",
    "            #wanted_df_1 = stock_info_df[stock_info_df['Date'] == date_string]\n",
    "            #wanted_df_2 = wanted_df_1[wanted_df_1['Ticker'] == buy_ticker_symbol]\n",
    "            #wanted_df_2 = wanted_df_2[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]]\n",
    "\n",
    "#@@@\n",
    "            #for w_column in wanted_df_2.columns.tolist():\n",
    "              #print(str(sell_ticker_symbol))\n",
    "              #print(str(w_column))\n",
    "              #print(str(wanted_df_2[w_column].values.tolist()[0])\n",
    "              #print(str(sell_ticker_symbol) + '_' + str(w_column)+ ' : ' + str(wanted_df_2[w_column].values.tolist()))\n",
    "\n",
    "            #  self.keep_row_info[str(buy_ticker_symbol) + '_' + str(w_column)] = str(wanted_df_2[w_column].values.tolist()[0])\n",
    "              #pass\n",
    "\n",
    "          else:\n",
    "\n",
    "            #************************\n",
    "            for idx in hold_indices:\n",
    "              #print(\"Stock to hold: \" + str(idx))\n",
    "              hold_ticker_symbol = self.ticker[idx]\n",
    "              #print('Ticker: ' + str(hold_ticker_symbol))\n",
    "\n",
    "              stock_to_hold_ticker = 'Stock_to_hold_' + str(hold_ticker_symbol)\n",
    "              self.keep_row_info[stock_to_hold_ticker] = 'Yes'\n",
    "\n",
    "              num_of_ticker_shares_available_for_holding = 'num_of_' + str(hold_ticker_symbol) + 'shares_available_for_holding'\n",
    "              self.keep_row_info[num_of_ticker_shares_available_for_holding] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "              #stock_info_df = self.df\n",
    "              #wanted_df_1 = stock_info_df[stock_info_df['Date'] == date_string]\n",
    "              #wanted_df_2 = wanted_df_1[wanted_df_1['Ticker'] == hold_ticker_symbol]\n",
    "              #wanted_df_2 = wanted_df_2[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]]\n",
    "\n",
    "#@@@\n",
    "              #for w_column in wanted_df_2.columns.tolist():\n",
    "                #print(str(sell_ticker_symbol))\n",
    "                #print(str(w_column))\n",
    "                #print(str(wanted_df_2[w_column].values.tolist()[0]))\n",
    "                #print(str(hold_ticker_symbol) + '_' + str(w_column)+ ' : ' + str(wanted_df_2[w_column].values.tolist()))\n",
    "\n",
    "              #  self.keep_row_info[str(hold_ticker_symbol) + '_' + str(w_column)] = str(wanted_df_2[w_column].values.tolist()[0])\n",
    "                #pass\n",
    "\n",
    "            # print('No Shares purchased')\n",
    "            pass\n",
    "        ###### Trading ends #######\n",
    "        # print('*************** Trading Ends ***************')\n",
    "        # print('Available cash for after trading : {}'.format(self.state[0]))\n",
    "        # print('Shares available per stock : '.format(np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1])))\n",
    "        # print('Over all portfolie before Trading : {}'.format(portfolio_after_trade))\n",
    "        # print('Profit or Loss for Day {} : {} is {}'.format(self.day, self.date_memory[-1], self.reward))\n",
    "\n",
    "        #************************\n",
    "        keep_available_cash_after_trading = self.state[0]\n",
    "        keep_shares_available_per_stock = np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1])\n",
    "        keep_profit_and_loss_for_day = self.reward\n",
    "        #keep_available_cash_after_trading_list.append(keep_available_cash_after_trading)\n",
    "        #keep_shares_available_per_stock_list.append(keep_shares_available_per_stock)\n",
    "        #keep_profit_and_loss_for_day_list.append(Keep_profit_and_loss_for_day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Setting the values for next trading date\n",
    "        self.day += 1\n",
    "\n",
    "        #************************\n",
    "\n",
    "\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "\n",
    "        self.state = [self.state[0]] + self.data.AdjClose.values.tolist() + list(self.state[(self.Action_Space+1):(2*self.Action_Space)+1]) + \\\n",
    "                      self.data.momentum_ppo.values.tolist() + self.data.momentum_rsi.values.tolist() + \\\n",
    "                      self.data.trend_adx.values.tolist() + self.data.trend_macd.values.tolist() + \\\n",
    "                      self.data.trend_cci.values.tolist()\n",
    "\n",
    "\n",
    "        portfolio_after_trade = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "\n",
    "        #************************\n",
    "        keep_portfolio_after_trade = portfolio_after_trade\n",
    "        #print('Portfolio value after trading:  ' + str(keep_portfolio_after_trade))\n",
    "        #print('Available Cash before trading:  ' + str(keep_available_cash_before_trading))\n",
    "        #print('Available Cash after trading:   ' + str(keep_available_cash_after_trading))\n",
    "        #print('Profit and loss for day:        ' + str(keep_profit_and_loss_for_day))\n",
    "\n",
    "        self.keep_row_info['portfolio_value_after_trading'] = str(keep_portfolio_after_trade)\n",
    "        self.keep_row_info['available_cash_before_trading'] = str(keep_available_cash_before_trading)\n",
    "        self.keep_row_info['available_cash_after_trading'] = str(keep_available_cash_after_trading)\n",
    "        self.keep_row_info['profit_and_loss_for_day'] = str(keep_profit_and_loss_for_day)\n",
    "\n",
    "        #Total trade in a day (profit or loss)\n",
    "        self.reward = portfolio_after_trade - portfolio_before_trade\n",
    "        #print('Day : {} | Reward : {}'.format(self.day-1, self.reward))\n",
    "        self.reward_memory.append(self.reward)\n",
    "        self.asset_memory.append(portfolio_after_trade)\n",
    "        self.date_memory.append(self.data.Date.unique()[0])\n",
    "        self.reward = self.reward*self.normalized_rewards     #Normalizing the reward\n",
    "\n",
    "        self.keep_rows_list.append(self.keep_row_info)\n",
    "        #print(self.keep_rows_list)\n",
    "\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "    def _seed(self, seed = 10):\n",
    "      randomState, seed = seeding.np_random(seed)\n",
    "      return [seed]\n",
    "\n",
    "    #added for seed\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "def render_trading(model, env, data, n_episodes = 1):\n",
    "  episode_rewards = [0.0]\n",
    "  obs = env.reset()\n",
    "  env.render()\n",
    "\n",
    "  for i in range(n_episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        #print(action)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        # Stats\n",
    "        if done:\n",
    "          obs = env.reset()\n",
    "          #print('Episode {} Rewards {}'.format(i+1, episode_rewards[-1]))\n",
    "          episode_rewards.append(0.0)\n",
    "        else:\n",
    "          #print(rewards)\n",
    "          episode_rewards[-1] += rewards\n",
    "    if (i+1)%100 == 0 and i != 0:\n",
    "      print('Average reward {}'.format(np.average(episode_rewards[:i+1])))\n",
    "\n",
    "  return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c886152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the date 2 days before today's date\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Convert string to datetime object\n",
    "date_object = datetime.strptime(todays_date, '%Y-%m-%d')\n",
    "\n",
    "# Subtract 2 days\n",
    "new_date_object = date_object - timedelta(days=2)\n",
    "\n",
    "# Convert back to string\n",
    "twoDaysBeforeTodays_date = new_date_object.strftime('%Y-%m-%d')\n",
    "\n",
    "stock_df = stock_data_tech.copy()\n",
    "#Training Data - 2009 --> end of 2020\n",
    "train_data = stock_df[(stock_df.Date >= '2009-01-01') & (stock_df.Date < '2024-04-30')].sort_values(by=['Date', 'Ticker'])\n",
    "\n",
    "#Validation Data 2021 --> mar 29 2022\n",
    "validation_data = stock_df[(stock_df.Date >= '2022-01-01') & (stock_df.Date < twoDaysBeforeTodays_date)]\n",
    "validation_data.sort_values([\"Date\", \"Ticker\"], ignore_index=True)\n",
    "validation_data.index = validation_data.Date.factorize()[0]\n",
    "\n",
    "#Trading Data - 2 days before current date\n",
    "trading_data = stock_df[(stock_df.Date >= twoDaysBeforeTodays_date)]\n",
    "trading_data.sort_values([\"Date\", \"Ticker\"], ignore_index=True)\n",
    "trading_data.index = trading_data.Date.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b9f5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-05-29'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoDaysBeforeTodays_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd4d7a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_ppo_signal</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>momentum_pvo_signal</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>189.610001</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>189.509995</td>\n",
       "      <td>190.289993</td>\n",
       "      <td>190.289993</td>\n",
       "      <td>53068000.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.279613e+13</td>\n",
       "      <td>3.631844e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.377611</td>\n",
       "      <td>2.435407</td>\n",
       "      <td>-0.057795</td>\n",
       "      <td>-9.055103</td>\n",
       "      <td>-7.380385</td>\n",
       "      <td>-1.674718</td>\n",
       "      <td>189.398764</td>\n",
       "      <td>0.157897</td>\n",
       "      <td>0.157772</td>\n",
       "      <td>6844.452754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>190.759995</td>\n",
       "      <td>192.179993</td>\n",
       "      <td>190.630005</td>\n",
       "      <td>191.289993</td>\n",
       "      <td>191.289993</td>\n",
       "      <td>49947900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.279614e+13</td>\n",
       "      <td>3.636838e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.326334</td>\n",
       "      <td>2.413592</td>\n",
       "      <td>-0.087258</td>\n",
       "      <td>-8.530273</td>\n",
       "      <td>-7.610363</td>\n",
       "      <td>-0.919910</td>\n",
       "      <td>189.435877</td>\n",
       "      <td>0.525514</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>6880.946805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>191.440002</td>\n",
       "      <td>192.570007</td>\n",
       "      <td>189.910004</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>74776921.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.279608e+13</td>\n",
       "      <td>3.644316e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.299492</td>\n",
       "      <td>2.390772</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>-4.263600</td>\n",
       "      <td>-6.941010</td>\n",
       "      <td>2.677410</td>\n",
       "      <td>189.520314</td>\n",
       "      <td>0.501859</td>\n",
       "      <td>0.500604</td>\n",
       "      <td>6915.981339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>181.699997</td>\n",
       "      <td>184.080002</td>\n",
       "      <td>181.550003</td>\n",
       "      <td>182.020004</td>\n",
       "      <td>182.020004</td>\n",
       "      <td>32009300.0</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2.219502e+10</td>\n",
       "      <td>1.803329e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026981</td>\n",
       "      <td>0.397175</td>\n",
       "      <td>-0.370194</td>\n",
       "      <td>-8.390572</td>\n",
       "      <td>-4.637696</td>\n",
       "      <td>-3.752877</td>\n",
       "      <td>182.471659</td>\n",
       "      <td>-0.071364</td>\n",
       "      <td>-0.071390</td>\n",
       "      <td>6596.836226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>181.309998</td>\n",
       "      <td>181.339996</td>\n",
       "      <td>178.360001</td>\n",
       "      <td>179.320007</td>\n",
       "      <td>179.320007</td>\n",
       "      <td>29249200.0</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2.218462e+10</td>\n",
       "      <td>1.800404e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137938</td>\n",
       "      <td>0.290152</td>\n",
       "      <td>-0.428090</td>\n",
       "      <td>-9.240997</td>\n",
       "      <td>-5.558356</td>\n",
       "      <td>-3.682641</td>\n",
       "      <td>181.940689</td>\n",
       "      <td>-1.483352</td>\n",
       "      <td>-1.494463</td>\n",
       "      <td>6497.498587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>178.229996</td>\n",
       "      <td>179.210007</td>\n",
       "      <td>173.869995</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>58350991.0</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2.218243e+10</td>\n",
       "      <td>1.794569e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.392337</td>\n",
       "      <td>0.153654</td>\n",
       "      <td>-0.545991</td>\n",
       "      <td>-3.457333</td>\n",
       "      <td>-5.138151</td>\n",
       "      <td>1.680818</td>\n",
       "      <td>180.963708</td>\n",
       "      <td>-1.606070</td>\n",
       "      <td>-1.619107</td>\n",
       "      <td>6391.538140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>17.240000</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>16.910000</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>16.920000</td>\n",
       "      <td>12268800.0</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>-4.957435e+11</td>\n",
       "      <td>1.172969e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686866</td>\n",
       "      <td>0.942282</td>\n",
       "      <td>-0.255416</td>\n",
       "      <td>-9.424412</td>\n",
       "      <td>-9.927313</td>\n",
       "      <td>0.502901</td>\n",
       "      <td>17.631396</td>\n",
       "      <td>-2.981654</td>\n",
       "      <td>-3.027009</td>\n",
       "      <td>-39.502739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.209999</td>\n",
       "      <td>16.959999</td>\n",
       "      <td>17.110001</td>\n",
       "      <td>17.110001</td>\n",
       "      <td>12306400.0</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>-4.957410e+11</td>\n",
       "      <td>1.185276e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582508</td>\n",
       "      <td>0.870328</td>\n",
       "      <td>-0.287819</td>\n",
       "      <td>-11.303681</td>\n",
       "      <td>-10.202586</td>\n",
       "      <td>-1.101095</td>\n",
       "      <td>17.622047</td>\n",
       "      <td>1.122935</td>\n",
       "      <td>1.116676</td>\n",
       "      <td>-38.823394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>16.969999</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>15459464.0</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>-4.957437e+11</td>\n",
       "      <td>1.169816e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484825</td>\n",
       "      <td>0.793227</td>\n",
       "      <td>-0.308402</td>\n",
       "      <td>-11.289881</td>\n",
       "      <td>-10.420045</td>\n",
       "      <td>-0.869836</td>\n",
       "      <td>17.607786</td>\n",
       "      <td>-0.116893</td>\n",
       "      <td>-0.116962</td>\n",
       "      <td>-38.894905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>176.809998</td>\n",
       "      <td>178.229996</td>\n",
       "      <td>176.259995</td>\n",
       "      <td>177.399994</td>\n",
       "      <td>177.399994</td>\n",
       "      <td>15023800.0</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>7.311319e+09</td>\n",
       "      <td>9.374980e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.549876</td>\n",
       "      <td>2.777653</td>\n",
       "      <td>-0.227777</td>\n",
       "      <td>-11.097125</td>\n",
       "      <td>-8.886391</td>\n",
       "      <td>-2.210734</td>\n",
       "      <td>174.172812</td>\n",
       "      <td>-0.348281</td>\n",
       "      <td>-0.348889</td>\n",
       "      <td>2116.667809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>176.690002</td>\n",
       "      <td>176.690002</td>\n",
       "      <td>173.229996</td>\n",
       "      <td>173.559998</td>\n",
       "      <td>173.559998</td>\n",
       "      <td>18844000.0</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>7.296069e+09</td>\n",
       "      <td>9.356136e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.262636</td>\n",
       "      <td>2.674649</td>\n",
       "      <td>-0.412014</td>\n",
       "      <td>-9.093435</td>\n",
       "      <td>-8.927800</td>\n",
       "      <td>-0.165635</td>\n",
       "      <td>174.169320</td>\n",
       "      <td>-2.164598</td>\n",
       "      <td>-2.188369</td>\n",
       "      <td>2068.685866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>173.192001</td>\n",
       "      <td>174.419998</td>\n",
       "      <td>170.970001</td>\n",
       "      <td>173.960007</td>\n",
       "      <td>173.960007</td>\n",
       "      <td>27173303.0</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>7.315997e+09</td>\n",
       "      <td>9.383309e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.030185</td>\n",
       "      <td>2.545757</td>\n",
       "      <td>-0.515571</td>\n",
       "      <td>-3.671716</td>\n",
       "      <td>-7.876583</td>\n",
       "      <td>4.204867</td>\n",
       "      <td>174.166383</td>\n",
       "      <td>0.230473</td>\n",
       "      <td>0.230208</td>\n",
       "      <td>2073.684105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>175.429993</td>\n",
       "      <td>176.839996</td>\n",
       "      <td>174.720001</td>\n",
       "      <td>175.899994</td>\n",
       "      <td>175.899994</td>\n",
       "      <td>23388700.0</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>7.746231e+09</td>\n",
       "      <td>1.147187e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.582747</td>\n",
       "      <td>2.803364</td>\n",
       "      <td>-0.220616</td>\n",
       "      <td>-10.492240</td>\n",
       "      <td>-8.726287</td>\n",
       "      <td>-1.765953</td>\n",
       "      <td>172.732771</td>\n",
       "      <td>-0.283447</td>\n",
       "      <td>-0.283849</td>\n",
       "      <td>2087.527532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>175.199997</td>\n",
       "      <td>175.220001</td>\n",
       "      <td>171.789993</td>\n",
       "      <td>172.110001</td>\n",
       "      <td>172.110001</td>\n",
       "      <td>22958700.0</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>7.727556e+09</td>\n",
       "      <td>1.144891e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.295920</td>\n",
       "      <td>2.701875</td>\n",
       "      <td>-0.405955</td>\n",
       "      <td>-9.576200</td>\n",
       "      <td>-8.896269</td>\n",
       "      <td>-0.679931</td>\n",
       "      <td>172.728897</td>\n",
       "      <td>-2.154630</td>\n",
       "      <td>-2.178181</td>\n",
       "      <td>2040.394417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>171.860001</td>\n",
       "      <td>173.059998</td>\n",
       "      <td>169.440002</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>36174235.0</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>7.752538e+09</td>\n",
       "      <td>1.148508e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063114</td>\n",
       "      <td>2.574123</td>\n",
       "      <td>-0.511008</td>\n",
       "      <td>-4.306067</td>\n",
       "      <td>-7.978229</td>\n",
       "      <td>3.672162</td>\n",
       "      <td>172.725041</td>\n",
       "      <td>0.226599</td>\n",
       "      <td>0.226343</td>\n",
       "      <td>2045.244527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>474.660004</td>\n",
       "      <td>479.850006</td>\n",
       "      <td>473.700012</td>\n",
       "      <td>474.359985</td>\n",
       "      <td>474.359985</td>\n",
       "      <td>9226200.0</td>\n",
       "      <td>META</td>\n",
       "      <td>-4.677814e+11</td>\n",
       "      <td>4.938794e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163512</td>\n",
       "      <td>-0.569173</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>-15.525547</td>\n",
       "      <td>-12.529452</td>\n",
       "      <td>-2.996096</td>\n",
       "      <td>470.551281</td>\n",
       "      <td>-1.158532</td>\n",
       "      <td>-1.165296</td>\n",
       "      <td>1142.122143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>471.670013</td>\n",
       "      <td>471.730011</td>\n",
       "      <td>464.709991</td>\n",
       "      <td>467.049988</td>\n",
       "      <td>467.049988</td>\n",
       "      <td>10735200.0</td>\n",
       "      <td>META</td>\n",
       "      <td>-4.677850e+11</td>\n",
       "      <td>4.938687e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235591</td>\n",
       "      <td>-0.502456</td>\n",
       "      <td>0.266865</td>\n",
       "      <td>-15.320114</td>\n",
       "      <td>-13.087584</td>\n",
       "      <td>-2.232530</td>\n",
       "      <td>470.345677</td>\n",
       "      <td>-1.541023</td>\n",
       "      <td>-1.553020</td>\n",
       "      <td>1122.980753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>465.799988</td>\n",
       "      <td>469.119995</td>\n",
       "      <td>454.460114</td>\n",
       "      <td>466.829987</td>\n",
       "      <td>466.829987</td>\n",
       "      <td>16645191.0</td>\n",
       "      <td>META</td>\n",
       "      <td>-4.677736e+11</td>\n",
       "      <td>4.938520e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293233</td>\n",
       "      <td>-0.460612</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>-11.198798</td>\n",
       "      <td>-12.709827</td>\n",
       "      <td>1.511029</td>\n",
       "      <td>470.257292</td>\n",
       "      <td>-0.047104</td>\n",
       "      <td>-0.047116</td>\n",
       "      <td>1122.404674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>425.690002</td>\n",
       "      <td>430.940002</td>\n",
       "      <td>425.690002</td>\n",
       "      <td>429.170013</td>\n",
       "      <td>429.170013</td>\n",
       "      <td>15517100.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-2.362918e+12</td>\n",
       "      <td>4.890425e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.300086</td>\n",
       "      <td>0.927833</td>\n",
       "      <td>0.372253</td>\n",
       "      <td>-6.394014</td>\n",
       "      <td>-5.251783</td>\n",
       "      <td>-1.142231</td>\n",
       "      <td>425.400004</td>\n",
       "      <td>-0.267242</td>\n",
       "      <td>-0.267599</td>\n",
       "      <td>2758.914256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>424.299988</td>\n",
       "      <td>424.299988</td>\n",
       "      <td>414.239990</td>\n",
       "      <td>414.670013</td>\n",
       "      <td>414.670013</td>\n",
       "      <td>28424800.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-2.362944e+12</td>\n",
       "      <td>4.862000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018286</td>\n",
       "      <td>0.945924</td>\n",
       "      <td>0.072363</td>\n",
       "      <td>-0.543692</td>\n",
       "      <td>-4.310165</td>\n",
       "      <td>3.766473</td>\n",
       "      <td>424.942515</td>\n",
       "      <td>-3.378614</td>\n",
       "      <td>-3.437009</td>\n",
       "      <td>2662.322566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>416.410004</td>\n",
       "      <td>416.630005</td>\n",
       "      <td>404.519989</td>\n",
       "      <td>415.130005</td>\n",
       "      <td>415.130005</td>\n",
       "      <td>47676297.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-2.362908e+12</td>\n",
       "      <td>4.909677e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.915615</td>\n",
       "      <td>-0.121235</td>\n",
       "      <td>10.867785</td>\n",
       "      <td>-1.274575</td>\n",
       "      <td>12.142360</td>\n",
       "      <td>424.664985</td>\n",
       "      <td>0.110930</td>\n",
       "      <td>0.110868</td>\n",
       "      <td>2665.386797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>858.630005</td>\n",
       "      <td>863.799988</td>\n",
       "      <td>827.090027</td>\n",
       "      <td>839.719971</td>\n",
       "      <td>839.719971</td>\n",
       "      <td>4932300.0</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>1.508469e+08</td>\n",
       "      <td>2.964848e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253577</td>\n",
       "      <td>-0.166280</td>\n",
       "      <td>0.419857</td>\n",
       "      <td>-0.706501</td>\n",
       "      <td>-2.316697</td>\n",
       "      <td>1.610197</td>\n",
       "      <td>875.082727</td>\n",
       "      <td>-4.001281</td>\n",
       "      <td>-4.083533</td>\n",
       "      <td>13000.155861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>845.239990</td>\n",
       "      <td>815.200012</td>\n",
       "      <td>827.940002</td>\n",
       "      <td>827.940002</td>\n",
       "      <td>4747600.0</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>1.501262e+08</td>\n",
       "      <td>2.917372e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093626</td>\n",
       "      <td>-0.151749</td>\n",
       "      <td>0.058123</td>\n",
       "      <td>-2.706719</td>\n",
       "      <td>-2.394702</td>\n",
       "      <td>-0.312017</td>\n",
       "      <td>867.877910</td>\n",
       "      <td>-1.402845</td>\n",
       "      <td>-1.412778</td>\n",
       "      <td>12816.381001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>815.770020</td>\n",
       "      <td>819.650024</td>\n",
       "      <td>760.010010</td>\n",
       "      <td>784.510010</td>\n",
       "      <td>784.510010</td>\n",
       "      <td>7168478.0</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>1.488473e+08</td>\n",
       "      <td>2.845687e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773555</td>\n",
       "      <td>-0.276110</td>\n",
       "      <td>-0.497445</td>\n",
       "      <td>-1.142673</td>\n",
       "      <td>-2.144296</td>\n",
       "      <td>1.001623</td>\n",
       "      <td>855.496104</td>\n",
       "      <td>-5.245548</td>\n",
       "      <td>-5.388136</td>\n",
       "      <td>12138.845998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>148.639999</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>147.740005</td>\n",
       "      <td>148.190002</td>\n",
       "      <td>148.190002</td>\n",
       "      <td>4880100.0</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>-5.101263e+10</td>\n",
       "      <td>1.063469e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.735024</td>\n",
       "      <td>0.049338</td>\n",
       "      <td>-0.784362</td>\n",
       "      <td>15.163323</td>\n",
       "      <td>7.012115</td>\n",
       "      <td>8.151208</td>\n",
       "      <td>157.461631</td>\n",
       "      <td>-1.691656</td>\n",
       "      <td>-1.706128</td>\n",
       "      <td>-41.641395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.699997</td>\n",
       "      <td>140.231003</td>\n",
       "      <td>140.949997</td>\n",
       "      <td>140.949997</td>\n",
       "      <td>9278600.0</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>-5.102012e+10</td>\n",
       "      <td>1.063377e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.494226</td>\n",
       "      <td>-0.259375</td>\n",
       "      <td>-1.234851</td>\n",
       "      <td>17.337300</td>\n",
       "      <td>9.077152</td>\n",
       "      <td>8.260148</td>\n",
       "      <td>153.857370</td>\n",
       "      <td>-4.885623</td>\n",
       "      <td>-5.009005</td>\n",
       "      <td>-44.492576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>140.270004</td>\n",
       "      <td>142.100006</td>\n",
       "      <td>133.589996</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>14004884.0</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>-5.102560e+10</td>\n",
       "      <td>1.063237e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.332818</td>\n",
       "      <td>-0.674064</td>\n",
       "      <td>-1.658754</td>\n",
       "      <td>23.442811</td>\n",
       "      <td>11.950284</td>\n",
       "      <td>11.492528</td>\n",
       "      <td>149.282159</td>\n",
       "      <td>-3.384182</td>\n",
       "      <td>-3.442771</td>\n",
       "      <td>-46.371048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>174.190002</td>\n",
       "      <td>178.149994</td>\n",
       "      <td>173.929993</td>\n",
       "      <td>176.190002</td>\n",
       "      <td>176.190002</td>\n",
       "      <td>54782600.0</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>8.769571e+10</td>\n",
       "      <td>1.250410e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.907224</td>\n",
       "      <td>0.980578</td>\n",
       "      <td>-0.073353</td>\n",
       "      <td>-11.557745</td>\n",
       "      <td>-8.811999</td>\n",
       "      <td>-2.745746</td>\n",
       "      <td>172.687458</td>\n",
       "      <td>-0.316830</td>\n",
       "      <td>-0.317333</td>\n",
       "      <td>10962.576437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>178.580002</td>\n",
       "      <td>182.669998</td>\n",
       "      <td>175.380005</td>\n",
       "      <td>178.789993</td>\n",
       "      <td>178.789993</td>\n",
       "      <td>77784800.0</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>8.769070e+10</td>\n",
       "      <td>1.251188e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926940</td>\n",
       "      <td>0.969850</td>\n",
       "      <td>-0.042910</td>\n",
       "      <td>-10.349022</td>\n",
       "      <td>-9.119403</td>\n",
       "      <td>-1.229618</td>\n",
       "      <td>172.796918</td>\n",
       "      <td>1.475674</td>\n",
       "      <td>1.464892</td>\n",
       "      <td>11125.824050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>178.419998</td>\n",
       "      <td>180.320007</td>\n",
       "      <td>173.820099</td>\n",
       "      <td>178.080002</td>\n",
       "      <td>178.080002</td>\n",
       "      <td>67110156.0</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>8.771155e+10</td>\n",
       "      <td>1.250517e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899642</td>\n",
       "      <td>0.955808</td>\n",
       "      <td>-0.056166</td>\n",
       "      <td>-10.414795</td>\n",
       "      <td>-9.378482</td>\n",
       "      <td>-1.036313</td>\n",
       "      <td>172.862631</td>\n",
       "      <td>-0.397109</td>\n",
       "      <td>-0.397900</td>\n",
       "      <td>11081.245274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>156.750000</td>\n",
       "      <td>156.880005</td>\n",
       "      <td>153.210007</td>\n",
       "      <td>154.339996</td>\n",
       "      <td>154.339996</td>\n",
       "      <td>15780000.0</td>\n",
       "      <td>TSM</td>\n",
       "      <td>-9.220384e+11</td>\n",
       "      <td>2.149413e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.291648</td>\n",
       "      <td>2.954873</td>\n",
       "      <td>0.336775</td>\n",
       "      <td>0.060285</td>\n",
       "      <td>-4.508800</td>\n",
       "      <td>4.569084</td>\n",
       "      <td>152.558243</td>\n",
       "      <td>-3.180483</td>\n",
       "      <td>-3.232159</td>\n",
       "      <td>2984.839717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>153.589996</td>\n",
       "      <td>154.050003</td>\n",
       "      <td>151.889999</td>\n",
       "      <td>152.960007</td>\n",
       "      <td>152.960007</td>\n",
       "      <td>9711000.0</td>\n",
       "      <td>TSM</td>\n",
       "      <td>-9.220385e+11</td>\n",
       "      <td>2.139702e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.999699</td>\n",
       "      <td>2.963838</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>-2.162780</td>\n",
       "      <td>-4.039596</td>\n",
       "      <td>1.876816</td>\n",
       "      <td>152.566857</td>\n",
       "      <td>-0.894123</td>\n",
       "      <td>-0.898144</td>\n",
       "      <td>2957.257451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>151.479996</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>147.809998</td>\n",
       "      <td>151.039993</td>\n",
       "      <td>151.039993</td>\n",
       "      <td>13738764.0</td>\n",
       "      <td>TSM</td>\n",
       "      <td>-9.220333e+11</td>\n",
       "      <td>2.125963e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.636883</td>\n",
       "      <td>2.898447</td>\n",
       "      <td>-0.261565</td>\n",
       "      <td>-1.417396</td>\n",
       "      <td>-3.515156</td>\n",
       "      <td>2.097760</td>\n",
       "      <td>152.550118</td>\n",
       "      <td>-1.255239</td>\n",
       "      <td>-1.263184</td>\n",
       "      <td>2918.881568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>272.869995</td>\n",
       "      <td>267.769989</td>\n",
       "      <td>271.619995</td>\n",
       "      <td>271.619995</td>\n",
       "      <td>13397900.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.189297e+09</td>\n",
       "      <td>9.838697e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.725777</td>\n",
       "      <td>-0.517374</td>\n",
       "      <td>-0.208403</td>\n",
       "      <td>19.407071</td>\n",
       "      <td>0.607698</td>\n",
       "      <td>18.799372</td>\n",
       "      <td>280.572864</td>\n",
       "      <td>0.663384</td>\n",
       "      <td>0.661193</td>\n",
       "      <td>3097.822356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>223.399994</td>\n",
       "      <td>225.089996</td>\n",
       "      <td>212.000000</td>\n",
       "      <td>218.009995</td>\n",
       "      <td>218.009995</td>\n",
       "      <td>66860900.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.194762e+09</td>\n",
       "      <td>9.170088e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.406149</td>\n",
       "      <td>-0.895129</td>\n",
       "      <td>-1.511020</td>\n",
       "      <td>57.728383</td>\n",
       "      <td>12.031835</td>\n",
       "      <td>45.696548</td>\n",
       "      <td>257.959282</td>\n",
       "      <td>-19.737133</td>\n",
       "      <td>-21.986310</td>\n",
       "      <td>2466.663893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>219.110001</td>\n",
       "      <td>234.619995</td>\n",
       "      <td>216.093002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>35984705.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.159477e+09</td>\n",
       "      <td>9.529935e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.244339</td>\n",
       "      <td>-1.364971</td>\n",
       "      <td>-1.879368</td>\n",
       "      <td>58.297647</td>\n",
       "      <td>21.284997</td>\n",
       "      <td>37.012649</td>\n",
       "      <td>254.292464</td>\n",
       "      <td>7.536355</td>\n",
       "      <td>7.265879</td>\n",
       "      <td>2660.096806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High         Low       Close    AdjClose  \\\n",
       "0 2024-05-29  189.610001  192.250000  189.509995  190.289993  190.289993   \n",
       "1 2024-05-30  190.759995  192.179993  190.630005  191.289993  191.289993   \n",
       "2 2024-05-31  191.440002  192.570007  189.910004  192.250000  192.250000   \n",
       "0 2024-05-29  181.699997  184.080002  181.550003  182.020004  182.020004   \n",
       "1 2024-05-30  181.309998  181.339996  178.360001  179.320007  179.320007   \n",
       "2 2024-05-31  178.229996  179.210007  173.869995  176.440002  176.440002   \n",
       "0 2024-05-29   17.240000   17.379999   16.910000   16.920000   16.920000   \n",
       "1 2024-05-30   16.959999   17.209999   16.959999   17.110001   17.110001   \n",
       "2 2024-05-31   17.129999   17.260000   16.969999   17.090000   17.090000   \n",
       "0 2024-05-29  176.809998  178.229996  176.259995  177.399994  177.399994   \n",
       "1 2024-05-30  176.690002  176.690002  173.229996  173.559998  173.559998   \n",
       "2 2024-05-31  173.192001  174.419998  170.970001  173.960007  173.960007   \n",
       "0 2024-05-29  175.429993  176.839996  174.720001  175.899994  175.899994   \n",
       "1 2024-05-30  175.199997  175.220001  171.789993  172.110001  172.110001   \n",
       "2 2024-05-31  171.860001  173.059998  169.440002  172.500000  172.500000   \n",
       "0 2024-05-29  474.660004  479.850006  473.700012  474.359985  474.359985   \n",
       "1 2024-05-30  471.670013  471.730011  464.709991  467.049988  467.049988   \n",
       "2 2024-05-31  465.799988  469.119995  454.460114  466.829987  466.829987   \n",
       "0 2024-05-29  425.690002  430.940002  425.690002  429.170013  429.170013   \n",
       "1 2024-05-30  424.299988  424.299988  414.239990  414.670013  414.670013   \n",
       "2 2024-05-31  416.410004  416.630005  404.519989  415.130005  415.130005   \n",
       "0 2024-05-29  858.630005  863.799988  827.090027  839.719971  839.719971   \n",
       "1 2024-05-30  832.000000  845.239990  815.200012  827.940002  827.940002   \n",
       "2 2024-05-31  815.770020  819.650024  760.010010  784.510010  784.510010   \n",
       "0 2024-05-29  148.639999  150.000000  147.740005  148.190002  148.190002   \n",
       "1 2024-05-30  147.000000  147.699997  140.231003  140.949997  140.949997   \n",
       "2 2024-05-31  140.270004  142.100006  133.589996  136.179993  136.179993   \n",
       "0 2024-05-29  174.190002  178.149994  173.929993  176.190002  176.190002   \n",
       "1 2024-05-30  178.580002  182.669998  175.380005  178.789993  178.789993   \n",
       "2 2024-05-31  178.419998  180.320007  173.820099  178.080002  178.080002   \n",
       "0 2024-05-29  156.750000  156.880005  153.210007  154.339996  154.339996   \n",
       "1 2024-05-30  153.589996  154.050003  151.889999  152.960007  152.960007   \n",
       "2 2024-05-31  151.479996  152.500000  147.809998  151.039993  151.039993   \n",
       "0 2024-05-29  268.000000  272.869995  267.769989  271.619995  271.619995   \n",
       "1 2024-05-30  223.399994  225.089996  212.000000  218.009995  218.009995   \n",
       "2 2024-05-31  219.110001  234.619995  216.093002  234.440002  234.440002   \n",
       "\n",
       "       Volume Ticker    volume_adi    volume_obv  ...  momentum_ppo  \\\n",
       "0  53068000.0   AAPL -1.279613e+13  3.631844e+10  ...      2.377611   \n",
       "1  49947900.0   AAPL -1.279614e+13  3.636838e+10  ...      2.326334   \n",
       "2  74776921.0   AAPL -1.279608e+13  3.644316e+10  ...      2.299492   \n",
       "0  32009300.0   AMZN  2.219502e+10  1.803329e+10  ...      0.026981   \n",
       "1  29249200.0   AMZN  2.218462e+10  1.800404e+10  ...     -0.137938   \n",
       "2  58350991.0   AMZN  2.218243e+10  1.794569e+10  ...     -0.392337   \n",
       "0  12268800.0   GOLD -4.957435e+11  1.172969e+09  ...      0.686866   \n",
       "1  12306400.0   GOLD -4.957410e+11  1.185276e+09  ...      0.582508   \n",
       "2  15459464.0   GOLD -4.957437e+11  1.169816e+09  ...      0.484825   \n",
       "0  15023800.0   GOOG  7.311319e+09  9.374980e+09  ...      2.549876   \n",
       "1  18844000.0   GOOG  7.296069e+09  9.356136e+09  ...      2.262636   \n",
       "2  27173303.0   GOOG  7.315997e+09  9.383309e+09  ...      2.030185   \n",
       "0  23388700.0  GOOGL  7.746231e+09  1.147187e+10  ...      2.582747   \n",
       "1  22958700.0  GOOGL  7.727556e+09  1.144891e+10  ...      2.295920   \n",
       "2  36174235.0  GOOGL  7.752538e+09  1.148508e+10  ...      2.063114   \n",
       "0   9226200.0   META -4.677814e+11  4.938794e+11  ...     -0.163512   \n",
       "1  10735200.0   META -4.677850e+11  4.938687e+11  ...     -0.235591   \n",
       "2  16645191.0   META -4.677736e+11  4.938520e+11  ...     -0.293233   \n",
       "0  15517100.0   MSFT -2.362918e+12  4.890425e+09  ...      1.300086   \n",
       "1  28424800.0   MSFT -2.362944e+12  4.862000e+09  ...      1.018286   \n",
       "2  47676297.0   MSFT -2.362908e+12  4.909677e+09  ...      0.794380   \n",
       "0   4932300.0   SMCI  1.508469e+08  2.964848e+08  ...      0.253577   \n",
       "1   4747600.0   SMCI  1.501262e+08  2.917372e+08  ...     -0.093626   \n",
       "2   7168478.0   SMCI  1.488473e+08  2.845687e+08  ...     -0.773555   \n",
       "0   4880100.0   SNOW -5.101263e+10  1.063469e+11  ...     -0.735024   \n",
       "1   9278600.0   SNOW -5.102012e+10  1.063377e+11  ...     -1.494226   \n",
       "2  14004884.0   SNOW -5.102560e+10  1.063237e+11  ...     -2.332818   \n",
       "0  54782600.0   TSLA  8.769571e+10  1.250410e+11  ...      0.907224   \n",
       "1  77784800.0   TSLA  8.769070e+10  1.251188e+11  ...      0.926940   \n",
       "2  67110156.0   TSLA  8.771155e+10  1.250517e+11  ...      0.899642   \n",
       "0  15780000.0    TSM -9.220384e+11  2.149413e+09  ...      3.291648   \n",
       "1   9711000.0    TSM -9.220385e+11  2.139702e+09  ...      2.999699   \n",
       "2  13738764.0    TSM -9.220333e+11  2.125963e+09  ...      2.636883   \n",
       "0  13397900.0    CRM -1.189297e+09  9.838697e+08  ...     -0.725777   \n",
       "1  66860900.0    CRM -1.194762e+09  9.170088e+08  ...     -2.406149   \n",
       "2  35984705.0    CRM -1.159477e+09  9.529935e+08  ...     -3.244339   \n",
       "\n",
       "   momentum_ppo_signal  momentum_ppo_hist  momentum_pvo  momentum_pvo_signal  \\\n",
       "0             2.435407          -0.057795     -9.055103            -7.380385   \n",
       "1             2.413592          -0.087258     -8.530273            -7.610363   \n",
       "2             2.390772          -0.091280     -4.263600            -6.941010   \n",
       "0             0.397175          -0.370194     -8.390572            -4.637696   \n",
       "1             0.290152          -0.428090     -9.240997            -5.558356   \n",
       "2             0.153654          -0.545991     -3.457333            -5.138151   \n",
       "0             0.942282          -0.255416     -9.424412            -9.927313   \n",
       "1             0.870328          -0.287819    -11.303681           -10.202586   \n",
       "2             0.793227          -0.308402    -11.289881           -10.420045   \n",
       "0             2.777653          -0.227777    -11.097125            -8.886391   \n",
       "1             2.674649          -0.412014     -9.093435            -8.927800   \n",
       "2             2.545757          -0.515571     -3.671716            -7.876583   \n",
       "0             2.803364          -0.220616    -10.492240            -8.726287   \n",
       "1             2.701875          -0.405955     -9.576200            -8.896269   \n",
       "2             2.574123          -0.511008     -4.306067            -7.978229   \n",
       "0            -0.569173           0.405660    -15.525547           -12.529452   \n",
       "1            -0.502456           0.266865    -15.320114           -13.087584   \n",
       "2            -0.460612           0.167378    -11.198798           -12.709827   \n",
       "0             0.927833           0.372253     -6.394014            -5.251783   \n",
       "1             0.945924           0.072363     -0.543692            -4.310165   \n",
       "2             0.915615          -0.121235     10.867785            -1.274575   \n",
       "0            -0.166280           0.419857     -0.706501            -2.316697   \n",
       "1            -0.151749           0.058123     -2.706719            -2.394702   \n",
       "2            -0.276110          -0.497445     -1.142673            -2.144296   \n",
       "0             0.049338          -0.784362     15.163323             7.012115   \n",
       "1            -0.259375          -1.234851     17.337300             9.077152   \n",
       "2            -0.674064          -1.658754     23.442811            11.950284   \n",
       "0             0.980578          -0.073353    -11.557745            -8.811999   \n",
       "1             0.969850          -0.042910    -10.349022            -9.119403   \n",
       "2             0.955808          -0.056166    -10.414795            -9.378482   \n",
       "0             2.954873           0.336775      0.060285            -4.508800   \n",
       "1             2.963838           0.035860     -2.162780            -4.039596   \n",
       "2             2.898447          -0.261565     -1.417396            -3.515156   \n",
       "0            -0.517374          -0.208403     19.407071             0.607698   \n",
       "1            -0.895129          -1.511020     57.728383            12.031835   \n",
       "2            -1.364971          -1.879368     58.297647            21.284997   \n",
       "\n",
       "   momentum_pvo_hist  momentum_kama  others_dr  others_dlr     others_cr  \n",
       "0          -1.674718     189.398764   0.157897    0.157772   6844.452754  \n",
       "1          -0.919910     189.435877   0.525514    0.524138   6880.946805  \n",
       "2           2.677410     189.520314   0.501859    0.500604   6915.981339  \n",
       "0          -3.752877     182.471659  -0.071364   -0.071390   6596.836226  \n",
       "1          -3.682641     181.940689  -1.483352   -1.494463   6497.498587  \n",
       "2           1.680818     180.963708  -1.606070   -1.619107   6391.538140  \n",
       "0           0.502901      17.631396  -2.981654   -3.027009    -39.502739  \n",
       "1          -1.101095      17.622047   1.122935    1.116676    -38.823394  \n",
       "2          -0.869836      17.607786  -0.116893   -0.116962    -38.894905  \n",
       "0          -2.210734     174.172812  -0.348281   -0.348889   2116.667809  \n",
       "1          -0.165635     174.169320  -2.164598   -2.188369   2068.685866  \n",
       "2           4.204867     174.166383   0.230473    0.230208   2073.684105  \n",
       "0          -1.765953     172.732771  -0.283447   -0.283849   2087.527532  \n",
       "1          -0.679931     172.728897  -2.154630   -2.178181   2040.394417  \n",
       "2           3.672162     172.725041   0.226599    0.226343   2045.244527  \n",
       "0          -2.996096     470.551281  -1.158532   -1.165296   1142.122143  \n",
       "1          -2.232530     470.345677  -1.541023   -1.553020   1122.980753  \n",
       "2           1.511029     470.257292  -0.047104   -0.047116   1122.404674  \n",
       "0          -1.142231     425.400004  -0.267242   -0.267599   2758.914256  \n",
       "1           3.766473     424.942515  -3.378614   -3.437009   2662.322566  \n",
       "2          12.142360     424.664985   0.110930    0.110868   2665.386797  \n",
       "0           1.610197     875.082727  -4.001281   -4.083533  13000.155861  \n",
       "1          -0.312017     867.877910  -1.402845   -1.412778  12816.381001  \n",
       "2           1.001623     855.496104  -5.245548   -5.388136  12138.845998  \n",
       "0           8.151208     157.461631  -1.691656   -1.706128    -41.641395  \n",
       "1           8.260148     153.857370  -4.885623   -5.009005    -44.492576  \n",
       "2          11.492528     149.282159  -3.384182   -3.442771    -46.371048  \n",
       "0          -2.745746     172.687458  -0.316830   -0.317333  10962.576437  \n",
       "1          -1.229618     172.796918   1.475674    1.464892  11125.824050  \n",
       "2          -1.036313     172.862631  -0.397109   -0.397900  11081.245274  \n",
       "0           4.569084     152.558243  -3.180483   -3.232159   2984.839717  \n",
       "1           1.876816     152.566857  -0.894123   -0.898144   2957.257451  \n",
       "2           2.097760     152.550118  -1.255239   -1.263184   2918.881568  \n",
       "0          18.799372     280.572864   0.663384    0.661193   3097.822356  \n",
       "1          45.696548     257.959282 -19.737133  -21.986310   2466.663893  \n",
       "2          37.012649     254.292464   7.536355    7.265879   2660.096806  \n",
       "\n",
       "[36 rows x 94 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd4957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30\n",
      "24\n",
      " \n",
      "2024-05-31\n",
      "12\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#if length od trading data is < 22, descend down the dates until 22\n",
    "\n",
    "trading_data = stock_df[(stock_df.Date >= twoDaysBeforeTodays_date)]\n",
    "current_date = twoDaysBeforeTodays_date\n",
    "if len(trading_data) > 22:\n",
    "    while len(trading_data) > 22:\n",
    "        ### Count down dates until there len(trading_data) >= 22 ###    \n",
    "    \n",
    "        # Convert string to datetime object\n",
    "        current_date_object = datetime.strptime(current_date, '%Y-%m-%d')\n",
    "        # Subtract 1 day\n",
    "        new_new_date_object = current_date_object + timedelta(days=1)\n",
    "        # Convert back to string\n",
    "        new_new_date = new_new_date_object.strftime('%Y-%m-%d')\n",
    "        current_date =  new_new_date\n",
    "\n",
    "        trading_data = stock_df[(stock_df.Date >= new_new_date)]    \n",
    "        trading_data.sort_values([\"Date\", \"Ticker\"], ignore_index=True)\n",
    "        trading_data.index = trading_data.Date.factorize()[0]\n",
    "    \n",
    "        print(current_date)\n",
    "        print(len(trading_data))\n",
    "        print(' ')\n",
    "        \n",
    "elif len(trading_data) < 22:\n",
    "    while len(trading_data) < 22:\n",
    "        ### Count down dates until there len(trading_data) >= 22 ###    \n",
    "    \n",
    "        # Convert string to datetime object\n",
    "        current_date_object = datetime.strptime(current_date, '%Y-%m-%d')\n",
    "        # Subtract 1 day\n",
    "        new_new_date_object = current_date_object - timedelta(days=1)\n",
    "        # Convert back to string\n",
    "        new_new_date = new_new_date_object.strftime('%Y-%m-%d')\n",
    "        current_date =  new_new_date\n",
    "        \n",
    "        trading_data = stock_df[(stock_df.Date >= new_new_date)]    \n",
    "        trading_data.sort_values([\"Date\", \"Ticker\"], ignore_index=True)\n",
    "        trading_data.index = trading_data.Date.factorize()[0]\n",
    "    \n",
    "        print(current_date)\n",
    "        print(len(trading_data))\n",
    "        print(' ')\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d586cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2024-05-31\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "0   2024-05-31\n",
      "Name: Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(trading_data['Date'].head(1))\n",
    "print(trading_data['Date'].tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "123cd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "new3_date = trading_data['Date'].head(1).iloc[0].strftime('%Y-%m-%d')\n",
    "twoDaysBeforeTodays_date = new3_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5713cecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-05-31'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoDaysBeforeTodays_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c14c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Environment\n",
    "train_env = DummyVecEnv([lambda : StockTradingEnv(train_data, Shares_Per_Trade,\n",
    "                                                  Initial_Investment, Action_Space,\n",
    "                                                  Observation_Space, verbosity=1, mode='train',\n",
    "                                                  Normalized_Rewards= Normalized_Rewards,\n",
    "                                                  commission = commission)])\n",
    "\n",
    "train_env = VecNormalize(train_env, norm_reward=False)\n",
    "\n",
    "#Validation Environment\n",
    "val_env = DummyVecEnv([lambda : StockTradingEnv(validation_data, Shares_Per_Trade,\n",
    "                                                Initial_Investment, Action_Space,\n",
    "                                                Observation_Space, verbosity=1, mode='val',\n",
    "                                                Normalized_Rewards= Normalized_Rewards,\n",
    "                                                commission = commission)])\n",
    "\n",
    "val_env = VecNormalize(val_env, norm_reward=False)\n",
    "\n",
    "#Trading Environment\n",
    "trade_env = DummyVecEnv([lambda : StockTradingEnv(trading_data, Shares_Per_Trade,\n",
    "                                                  Initial_Investment, Action_Space,\n",
    "                                                  Observation_Space, verbosity=1, mode='trade',\n",
    "                                                  Normalized_Rewards= Normalized_Rewards,\n",
    "                                                  commission = commission)])\n",
    "\n",
    "trade_env = VecNormalize(trade_env, norm_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43df2f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./logs\\A2C_250\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1378054.6353661441 | Total rewards : 1358054.6353661441 | % of profit : 6790.273176830721\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2581834.3919900176 | Total rewards : 2561834.3919900176 | % of profit : 12809.171959950088\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2478075.233029923 | Total rewards : 2458075.233029923 | % of profit : 12290.376165149613\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2343305.0760615375 | Total rewards : 2323305.0760615375 | % of profit : 11616.525380307688\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2894919.8690124396 | Total rewards : 2874919.8690124396 | % of profit : 14374.599345062199\n",
      "Eval num_timesteps=2000, episode_reward=23283.87 +/- 5157.92\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.33e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.00226 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -210     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.05e+03 |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1261988.782537443 | Total rewards : 1241988.782537443 | % of profit : 6209.943912687215\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1330521.5135463723 | Total rewards : 1310521.5135463723 | % of profit : 6552.607567731862\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1308127.807840035 | Total rewards : 1288127.807840035 | % of profit : 6440.639039200176\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1403522.921857011 | Total rewards : 1383522.921857011 | % of profit : 6917.614609285055\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1338215.5094263079 | Total rewards : 1318215.5094263079 | % of profit : 6591.077547131539\n",
      "Eval num_timesteps=4000, episode_reward=13122.73 +/- 462.24\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.31e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0285   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -3       |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 222      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 115      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0443  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -3.65    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 234      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1684487.5832575648 | Total rewards : 1664487.5832575648 | % of profit : 8322.437916287825\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1664881.329041284 | Total rewards : 1644881.329041284 | % of profit : 8224.40664520642\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1692841.378010468 | Total rewards : 1672841.378010468 | % of profit : 8364.20689005234\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1703902.800969698 | Total rewards : 1683902.800969698 | % of profit : 8419.51400484849\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1694088.3294779132 | Total rewards : 1674088.3294779132 | % of profit : 8370.441647389565\n",
      "Eval num_timesteps=6000, episode_reward=16762.52 +/- 132.59\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.68e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | 0.0283   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -386     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.19e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1641696.9188236552 | Total rewards : 1621696.9188236552 | % of profit : 8108.484594118277\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1597111.935249978 | Total rewards : 1577111.935249978 | % of profit : 7885.55967624989\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1547389.742955506 | Total rewards : 1527389.742955506 | % of profit : 7636.948714777531\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1511534.3108239798 | Total rewards : 1491534.3108239798 | % of profit : 7457.671554119898\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1478945.4646840345 | Total rewards : 1458945.4646840345 | % of profit : 7294.727323420173\n",
      "Eval num_timesteps=8000, episode_reward=15427.21 +/- 587.77\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.54e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | 0.033    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 305      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1972953.5759288534 | Total rewards : 1952953.5759288534 | % of profit : 9764.767879644267\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1996978.0528714934 | Total rewards : 1976978.0528714934 | % of profit : 9884.890264357467\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1997509.186993146 | Total rewards : 1977509.186993146 | % of profit : 9887.54593496573\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2106296.475991282 | Total rewards : 2086296.4759912821 | % of profit : 10431.482379956411\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2076141.1894161184 | Total rewards : 2056141.1894161184 | % of profit : 10280.705947080593\n",
      "Eval num_timesteps=10000, episode_reward=20010.07 +/- 520.18\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2e+04    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0438   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 17.7     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 26.3     |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 94    |\n",
      "|    iterations      | 2000  |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 10000 |\n",
      "------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1653332.5804488768 | Total rewards : 1633332.5804488768 | % of profit : 8166.662902244385\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1681850.6175868893 | Total rewards : 1661850.6175868893 | % of profit : 8309.253087934447\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1684473.5697211851 | Total rewards : 1664473.5697211851 | % of profit : 8322.367848605927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1666602.0527273323 | Total rewards : 1646602.0527273323 | % of profit : 8233.010263636663\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1636922.4248760287 | Total rewards : 1616922.4248760287 | % of profit : 8084.612124380143\n",
      "Eval num_timesteps=12000, episode_reward=16528.28 +/- 179.63\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.65e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0477   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -399     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.27e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1402149.0719232403 | Total rewards : 1382149.0719232403 | % of profit : 6910.745359616202\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1301993.0698514497 | Total rewards : 1281993.0698514497 | % of profit : 6409.965349257249\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1272732.8308939135 | Total rewards : 1252732.8308939135 | % of profit : 6263.664154469568\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1257786.2974071773 | Total rewards : 1237786.2974071773 | % of profit : 6188.931487035886\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1259253.5704536997 | Total rewards : 1239253.5704536997 | % of profit : 6196.267852268498\n",
      "Eval num_timesteps=14000, episode_reward=12842.68 +/- 544.68\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.28e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0166   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 116      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.79e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0424  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 286      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 480      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2379199.6287976564 | Total rewards : 2359199.6287976564 | % of profit : 11795.998143988281\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2322888.9864682457 | Total rewards : 2302888.9864682457 | % of profit : 11514.444932341228\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2151092.649086122 | Total rewards : 2131092.649086122 | % of profit : 10655.46324543061\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2112859.698911431 | Total rewards : 2092859.6989114308 | % of profit : 10464.298494557153\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2012276.0845089273 | Total rewards : 1992276.0845089273 | % of profit : 9961.380422544637\n",
      "Eval num_timesteps=16000, episode_reward=21894.65 +/- 1369.18\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.19e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0645   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -38.5    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 864      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1265841.6562585402 | Total rewards : 1245841.6562585402 | % of profit : 6229.208281292701\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1262729.1646923656 | Total rewards : 1242729.1646923656 | % of profit : 6213.645823461828\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1263536.9058517385 | Total rewards : 1243536.9058517385 | % of profit : 6217.684529258692\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1246641.6440828764 | Total rewards : 1226641.6440828764 | % of profit : 6133.208220414383\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1246199.6422362595 | Total rewards : 1226199.6422362595 | % of profit : 6130.998211181298\n",
      "Eval num_timesteps=18000, episode_reward=12421.69 +/- 87.55\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.24e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0369   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 126      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.8e+03  |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1160125.7035780484 | Total rewards : 1140125.7035780484 | % of profit : 5700.628517890242\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1163930.0365668612 | Total rewards : 1143930.0365668612 | % of profit : 5719.650182834306\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1164133.7886529425 | Total rewards : 1144133.7886529425 | % of profit : 5720.668943264713\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1194869.8951715254 | Total rewards : 1174869.8951715254 | % of profit : 5874.349475857627\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1214996.4647643182 | Total rewards : 1194996.4647643182 | % of profit : 5974.98232382159\n",
      "Eval num_timesteps=20000, episode_reward=11642.15 +/- 218.40\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.16e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0524   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -23.9    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 770      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 94    |\n",
      "|    iterations      | 4000  |\n",
      "|    time_elapsed    | 212   |\n",
      "|    total_timesteps | 20000 |\n",
      "------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1189061.7021854823 | Total rewards : 1169061.7021854823 | % of profit : 5845.308510927412\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1203554.8008496717 | Total rewards : 1183554.8008496717 | % of profit : 5917.774004248358\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1219490.2566284954 | Total rewards : 1199490.2566284954 | % of profit : 5997.451283142477\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1242580.6204784934 | Total rewards : 1222580.6204784934 | % of profit : 6112.903102392467\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1247356.7648767515 | Total rewards : 1227356.7648767515 | % of profit : 6136.783824383757\n",
      "Eval num_timesteps=22000, episode_reward=12053.16 +/- 224.60\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.21e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.035    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -73.8    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.33e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1249016.1348215938 | Total rewards : 1229016.1348215938 | % of profit : 6145.080674107969\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1249987.323712637 | Total rewards : 1229987.323712637 | % of profit : 6149.936618563185\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1251382.6761536445 | Total rewards : 1231382.6761536445 | % of profit : 6156.913380768222\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1252166.8139146667 | Total rewards : 1232166.8139146667 | % of profit : 6160.834069573333\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1252360.5264100055 | Total rewards : 1232360.5264100055 | % of profit : 6161.802632050028\n",
      "Eval num_timesteps=24000, episode_reward=12361.14 +/- 12.98\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.24e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0121   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 114      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.63e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 97       |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 256      |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0204   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -163     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 598      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1150111.0851530437 | Total rewards : 1130111.0851530437 | % of profit : 5650.555425765218\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1152018.5839499694 | Total rewards : 1132018.5839499694 | % of profit : 5660.092919749847\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1153718.5178532547 | Total rewards : 1133718.5178532547 | % of profit : 5668.5925892662735\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1152848.9240536743 | Total rewards : 1132848.9240536743 | % of profit : 5664.244620268371\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1154571.5467589265 | Total rewards : 1134571.5467589265 | % of profit : 5672.857733794633\n",
      "Eval num_timesteps=26000, episode_reward=11370.60 +/- 15.42\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.14e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0491   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -97.4    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 769      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 900831.910603306 | Total rewards : 880831.910603306 | % of profit : 4404.15955301653\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 901229.9145782113 | Total rewards : 881229.9145782113 | % of profit : 4406.149572891057\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 897575.8000414092 | Total rewards : 877575.8000414092 | % of profit : 4387.8790002070455\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 895509.526074599 | Total rewards : 875509.526074599 | % of profit : 4377.547630372995\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 893911.0348107088 | Total rewards : 873911.0348107088 | % of profit : 4369.555174053544\n",
      "Eval num_timesteps=28000, episode_reward=8803.30 +/- 28.98\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 8.8e+03  |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0371   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -297     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.92e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1156029.9222656777 | Total rewards : 1136029.9222656777 | % of profit : 5680.149611328388\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1157624.1845070468 | Total rewards : 1137624.1845070468 | % of profit : 5688.120922535234\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1154765.7812824552 | Total rewards : 1134765.7812824552 | % of profit : 5673.8289064122755\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1160838.571974541 | Total rewards : 1140838.571974541 | % of profit : 5704.192859872705\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1164249.3801471484 | Total rewards : 1144249.3801471484 | % of profit : 5721.246900735742\n",
      "Eval num_timesteps=30000, episode_reward=11431.52 +/- 34.64\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.14e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0736   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -31.6    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.42e+03 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 94    |\n",
      "|    iterations      | 6000  |\n",
      "|    time_elapsed    | 318   |\n",
      "|    total_timesteps | 30000 |\n",
      "------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1531747.8332024007 | Total rewards : 1511747.8332024007 | % of profit : 7558.739166012004\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1544615.5171145645 | Total rewards : 1524615.5171145645 | % of profit : 7623.0775855728225\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1560792.8559055014 | Total rewards : 1540792.8559055014 | % of profit : 7703.964279527508\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1641957.5056658527 | Total rewards : 1621957.5056658527 | % of profit : 8109.787528329263\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1603726.298211727 | Total rewards : 1583726.298211727 | % of profit : 7918.631491058636\n",
      "Eval num_timesteps=32000, episode_reward=15641.22 +/- 410.23\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.56e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0775   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -62.5    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 439      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1679642.6778159481 | Total rewards : 1659642.6778159481 | % of profit : 8298.213389079741\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1688831.5306503407 | Total rewards : 1668831.5306503407 | % of profit : 8344.157653251703\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1646795.5196977549 | Total rewards : 1626795.5196977549 | % of profit : 8133.977598488775\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1651641.7102945582 | Total rewards : 1631641.7102945582 | % of profit : 8158.2085514727905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1653673.1533735339 | Total rewards : 1633673.1533735339 | % of profit : 8168.365766867669\n",
      "Eval num_timesteps=34000, episode_reward=16523.11 +/- 169.56\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.65e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0188   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -194     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 5e+03    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 96       |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 362      |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0714   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 201      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 365      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1527443.590789171 | Total rewards : 1507443.590789171 | % of profit : 7537.217953945854\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1541730.1085338504 | Total rewards : 1521730.1085338504 | % of profit : 7608.650542669252\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1542981.2498922 | Total rewards : 1522981.2498922 | % of profit : 7614.906249461002\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1540334.1499915444 | Total rewards : 1520334.1499915444 | % of profit : 7601.670749957722\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1525184.9580771879 | Total rewards : 1505184.9580771879 | % of profit : 7525.92479038594\n",
      "Eval num_timesteps=36000, episode_reward=15227.76 +/- 76.64\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.52e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0624   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -68.5    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.79e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2195798.230049648 | Total rewards : 2175798.230049648 | % of profit : 10878.99115024824\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2201756.5917449496 | Total rewards : 2181756.5917449496 | % of profit : 10908.782958724747\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2155836.831378147 | Total rewards : 2135836.831378147 | % of profit : 10679.184156890735\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2115184.4452007106 | Total rewards : 2095184.4452007106 | % of profit : 10475.922226003553\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2063448.884621527 | Total rewards : 2043448.884621527 | % of profit : 10217.244423107635\n",
      "Eval num_timesteps=38000, episode_reward=21381.67 +/- 522.46\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.14e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0636   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -123     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.79e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2211309.3473080946 | Total rewards : 2191309.3473080946 | % of profit : 10956.546736540473\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2217220.9945352646 | Total rewards : 2197220.9945352646 | % of profit : 10986.104972676323\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2314226.716977814 | Total rewards : 2294226.716977814 | % of profit : 11471.13358488907\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2268206.0237961113 | Total rewards : 2248206.0237961113 | % of profit : 11241.030118980556\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2294058.0530765294 | Total rewards : 2274058.0530765294 | % of profit : 11370.290265382648\n",
      "Eval num_timesteps=40000, episode_reward=22536.17 +/- 412.02\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.25e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0719   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -286     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 2.77e+03 |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 94    |\n",
      "|    iterations      | 8000  |\n",
      "|    time_elapsed    | 425   |\n",
      "|    total_timesteps | 40000 |\n",
      "------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1605630.0199253047 | Total rewards : 1585630.0199253047 | % of profit : 7928.150099626523\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1606734.3140955104 | Total rewards : 1586734.3140955104 | % of profit : 7933.671570477552\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1608476.5409236497 | Total rewards : 1588476.5409236497 | % of profit : 7942.382704618249\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1604272.9356317678 | Total rewards : 1584272.9356317678 | % of profit : 7921.364678158839\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1604313.7768996207 | Total rewards : 1584313.7768996207 | % of profit : 7921.568884498103\n",
      "Eval num_timesteps=42000, episode_reward=15936.45 +/- 15.99\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 1.59e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0881   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -113     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.13e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2388398.393406418 | Total rewards : 2368398.393406418 | % of profit : 11841.99196703209\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2381153.7802167214 | Total rewards : 2361153.7802167214 | % of profit : 11805.768901083606\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2364331.354455972 | Total rewards : 2344331.354455972 | % of profit : 11721.65677227986\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2333026.2849296555 | Total rewards : 2313026.2849296555 | % of profit : 11565.131424648278\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2081396.1877906164 | Total rewards : 2061396.1877906164 | % of profit : 10306.980938953082\n",
      "Eval num_timesteps=44000, episode_reward=23026.37 +/- 1165.66\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.3e+04  |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0496   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -8.19    |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.8e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 95       |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 470      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.23     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 102      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 221      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2846021.08740098 | Total rewards : 2826021.08740098 | % of profit : 14130.1054370049\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2848785.706541843 | Total rewards : 2828785.706541843 | % of profit : 14143.928532709215\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2849517.480747158 | Total rewards : 2829517.480747158 | % of profit : 14147.58740373579\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2872650.018584746 | Total rewards : 2852650.018584746 | % of profit : 14263.25009292373\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2879980.9563205843 | Total rewards : 2859980.9563205843 | % of profit : 14299.904781602922\n",
      "Eval num_timesteps=46000, episode_reward=28503.70 +/- 131.01\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.85e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0954   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -225     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.78e+03 |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2760542.068990155 | Total rewards : 2740542.068990155 | % of profit : 13702.710344950774\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2982575.1655111336 | Total rewards : 2962575.1655111336 | % of profit : 14812.875827555668\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2757088.747011883 | Total rewards : 2737088.747011883 | % of profit : 13685.443735059416\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 3006839.8292340576 | Total rewards : 2986839.8292340576 | % of profit : 14934.19914617029\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2693801.3366297246 | Total rewards : 2673801.3366297246 | % of profit : 13369.006683148622\n",
      "Eval num_timesteps=48000, episode_reward=28354.54 +/- 1309.80\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.84e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.1      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -361     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.88e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2585842.440835236 | Total rewards : 2565842.440835236 | % of profit : 12829.21220417618\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2758988.596010407 | Total rewards : 2738988.596010407 | % of profit : 13694.942980052036\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2717525.863390017 | Total rewards : 2697525.863390017 | % of profit : 13487.629316950082\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2872864.51474337 | Total rewards : 2852864.51474337 | % of profit : 14264.322573716849\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2731615.563781796 | Total rewards : 2711615.563781796 | % of profit : 13558.07781890898\n",
      "Eval num_timesteps=50000, episode_reward=27281.21 +/- 904.34\n",
      "Episode length: 3856.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.86e+03 |\n",
      "|    mean_reward        | 2.73e+04 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.184    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | -187     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 789      |\n",
      "------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 94    |\n",
      "|    iterations      | 10000 |\n",
      "|    time_elapsed    | 531   |\n",
      "|    total_timesteps | 50000 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x23441d7e7d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **** Setup A2C Model ****\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback,StopTrainingOnRewardThreshold\n",
    "\n",
    "a2c_model = A2C('MlpPolicy', env = train_env, seed= 10,\n",
    "                tensorboard_log= './logs',\n",
    "                verbose= 1, learning_rate= 0.0005, ent_coef= 0.05, n_steps=5, gamma=0.1,max_grad_norm=0.5)\n",
    "\n",
    "logdir = os.path.join(\"/content/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Setup the evaluation callback\n",
    "save_path=r\"/content/best_model\"\n",
    "stop_callback=StopTrainingOnRewardThreshold(reward_threshold=50000,verbose=1)\n",
    "eval_callback = EvalCallback(eval_env=train_env,callback_on_new_best=stop_callback, best_model_save_path=save_path,\n",
    "                             log_path='./logs', eval_freq=2000,verbose=1,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# Pass the callback to the model's learn method\n",
    "a2c_model.learn(total_timesteps=50000, log_interval= 1000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a11e8",
   "metadata": {},
   "source": [
    "# Optimize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff708e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilvergreen333\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connnect to Weights and Biases\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "#fa012b2b2fd4bb72581ff4f975f151c2c74bae6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88808589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\silve\\OneDrive\\Desktop\\STUDY\\RL AlgorithmicTrading\\Colab STuff\\0421_Brian\\0421_Brian\\wandb\\run-20240531_172936-4gu9cyq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/4gu9cyq5' target=\"_blank\">elated-disco-736</a></strong> to <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment' target=\"_blank\">https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/4gu9cyq5' target=\"_blank\">https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/4gu9cyq5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to runs/4gu9cyq5\\A2C_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | -0.00878 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -19.2    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 98.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | -0.0135  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -28.5    |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 159      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | 0.00192  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -224     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 756      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00362  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -294     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.65e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0055   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 585      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 5.58e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00125  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 142      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.73e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00249  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -864     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.86e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1091963.1300100507 | Total rewards : 1071963.1300100507 | % of profit : 5359.815650050254\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.042    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 75.8     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 45.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0276   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 64.1     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 76.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.018    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 143      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 510      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0327   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -40.6    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 123      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0224   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -289     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 742      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0492   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -93      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 325      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0137   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 329      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.44e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0117   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 25.8     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.64e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 535691.0720697702 | Total rewards : 515691.0720697702 | % of profit : 2578.4553603488507\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | 0.0133   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -30.1    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 57.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.117   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 41.4     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 103      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0654   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 29.9     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 161      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | 0.0156   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -100     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 594      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | -0.0235  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -150     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 384      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0136  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 170      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 204      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00313  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -285     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.04e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.00054 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -594     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.21e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 516599.0227874679 | Total rewards : 496599.0227874679 | % of profit : 2482.9951139373397\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0698  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 91.4     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 35       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00522  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 64.2     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 199      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0257   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -42.4    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 214      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.00128 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 182      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 459      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0113   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 53.7     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 305      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00239  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 939      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 7.52e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 617      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00162  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 373      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.13e+04 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 863244.6013850659 | Total rewards : 843244.6013850659 | % of profit : 4216.223006925329\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 616      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.0854  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -26.2    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 26.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 616      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.198    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -30.1    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 16.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 618      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0268   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -33.5    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 51.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 619      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0127   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -53.2    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 231      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0601   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -18      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 188      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.0499  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 323      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 767      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00734  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 876      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 5.44e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00421  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -632     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.65e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 859248.9177916665 | Total rewards : 839248.9177916665 | % of profit : 4196.244588958332\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.0741  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 136      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 85.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 624      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0428   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 17.7     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 108      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 624      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0967   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 472      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.36e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 625      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0383   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -55.1    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 884      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.018    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -66      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 30.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.031    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 139      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 134      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0194   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 447      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.11e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.00086 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -1.13    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.14e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 439306.64155202184 | Total rewards : 419306.64155202184 | % of profit : 2096.5332077601092\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0486   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -152     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 212      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.081   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -86.1    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 106      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00954  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 249      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 539      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0637   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -17.9    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0154  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -327     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 878      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00529  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 304      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 7.59e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 631       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.2     |\n",
      "|    explained_variance | 0.00635   |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -1.53e+03 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.93e+04  |\n",
      "-------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 597871.5313239382 | Total rewards : 577871.5313239382 | % of profit : 2889.3576566196907\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 631       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.2     |\n",
      "|    explained_variance | -0.0264   |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -1.03e+03 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.65e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0835  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -22.8    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 62.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0303   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -95.2    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 188      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0122   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 76.2     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.12e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0026   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 23.7     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 799      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.018    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -760     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.05e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.00142 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 3.66e+03 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 9.3e+04  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0013   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 628      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.26e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1121576.948741096 | Total rewards : 1101576.948741096 | % of profit : 5507.88474370548\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.16     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 7.52     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 11.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.000741 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 215      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 534      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 50       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0157   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 12.7     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.06e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 636      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0138   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 740      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3e+03    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 636      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0156  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 588      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.68e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00638  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 778      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.82e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.00594 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 1.98e+03 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.14e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00991  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 1.42e+03 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 4.61e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 2327611.967482413 | Total rewards : 2307611.967482413 | % of profit : 11538.059837412065\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.112    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 43.1     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 12.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.103    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 21       |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 35.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.116   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 65.8     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 144      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.0672  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | -132     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 180      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0199   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -856     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 6.1e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0229   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 896      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 9.13e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.00283  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -303     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.05e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.0375  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -133     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 861      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 415594.0919536078 | Total rewards : 395594.0919536078 | % of profit : 1977.970459768039\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0756   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 163      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 335      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0889   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 236      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 647      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.00475  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -213     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.81e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.00662  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -218     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 4.86e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.00121  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | 241      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 3.86e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.00408  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 3.51e+03 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.43e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 640       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.4     |\n",
      "|    explained_variance | 0.00718   |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -3.24e+03 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.34e+05  |\n",
      "-------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 3485201.1752859885 | Total rewards : 3465201.1752859885 | % of profit : 17326.005876429943\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.25     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 55.2     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 41.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0353   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -398     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.54e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.032   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -184     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 496      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.126    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -61.1    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 260      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | -0.0364  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 3.41     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 461      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0317   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 753      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 3.41e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0176   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -830     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.88e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0114   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -150     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 2.2e+04  |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1458679.764356705 | Total rewards : 1438679.764356705 | % of profit : 7193.398821783525\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0455   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 25       |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 33.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0734   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -92.7    |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 78       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.143    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -34      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 431      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0149   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | 59       |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.06e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 643      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0148   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 1.24e+03 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 2.93e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 643      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0255   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -193     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 2.39e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 643      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0158   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 222      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 5.25e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 643       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.5     |\n",
      "|    explained_variance | 0.00444   |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -2.27e+03 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.08e+05  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.210 MB of 0.210 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>time/fps</td><td>â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/entropy_loss</td><td>â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–„â–„â–„â–…â–…â–…â–„â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>train/explained_variance</td><td>â–…â–…â–…â–†â–†â–†â–…â–‡â–„â–…â–†â–…â–‚â–…â–ƒâ–‚â–ˆâ–†â–…â–…â–„â–„â–†â–…â–…â–…â–…â–…â–â–…â–„â–ˆâ–…â–…â–„â–„â–…â–‡â–…â–…</td></tr><tr><td>train/learning_rate</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/policy_loss</td><td>â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–…â–„â–†â–…â–‡â–†â–‡â–ˆâ–†â–…â–†â–†â–†â–â–†â–†â–†â–†â–ˆâ–‚</td></tr><tr><td>train/std</td><td>â–â–â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/value_loss</td><td>â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–‚â–ˆâ–â–â–‚â–â–‚â–„</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>50000</td></tr><tr><td>time/fps</td><td>643.0</td></tr><tr><td>train/entropy_loss</td><td>-17.49494</td></tr><tr><td>train/explained_variance</td><td>0.00444</td></tr><tr><td>train/learning_rate</td><td>0.0005</td></tr><tr><td>train/policy_loss</td><td>-2268.77808</td></tr><tr><td>train/std</td><td>1.0413</td></tr><tr><td>train/value_loss</td><td>108295.14062</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-disco-736</strong> at: <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/4gu9cyq5' target=\"_blank\">https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/4gu9cyq5</a><br/> View project at: <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment' target=\"_blank\">https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240531_172936-4gu9cyq5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wandb.integration.sb3 import WandbCallback\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 50000,\n",
    "    \"env_name\": train_env,\n",
    "}\n",
    "\n",
    "hyperparams = dict(\n",
    "    n_steps=5, # number of steps to collect data before updating policy\n",
    "    learning_rate=0.0005,\n",
    "    gamma=0.1, # discount factor\n",
    "    max_grad_norm=0.5, # The maximum value for the gradient clipping\n",
    "    ent_coef=0.05, # Entropy coefficient for the loss calculation\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "hyperparams = dict(\n",
    "    n_steps=7, # number of steps to collect data before updating policy\n",
    "    learning_rate=0.001003582768121682,\n",
    "    gamma=0.0018126757076625249, # discount factor\n",
    "    max_grad_norm=0.2819622027161751, # The maximum value for the gradient clipping\n",
    "    ent_coef=0.05, # Entropy coefficient for the loss calculation\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"20240421_US_STOCK_experiment\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "a2c_model=A2C(\"MlpPolicy\", train_env, seed=10, verbose=1, **hyperparams,tensorboard_log=f\"runs/{run.id}\")\n",
    "\n",
    "a2c_model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"wb_logs/{run.id}\", # save the model in the run directory\n",
    "        verbose=2,\n",
    "    ),\n",
    ")\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "341c8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(a2c_model, trade_env, n_eval_episodes=100, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e486f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\silve\\OneDrive\\Desktop\\STUDY\\RL AlgorithmicTrading\\Colab STuff\\0421_Brian\\0421_Brian\\wandb\\run-20240531_173118-u95z3rv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/u95z3rv8' target=\"_blank\">decent-morning-737</a></strong> to <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment' target=\"_blank\">https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/u95z3rv8' target=\"_blank\">https://wandb.ai/silvergreen333/20240421_US_STOCK_experiment/runs/u95z3rv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to runs/u95z3rv8\\A2C_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 652      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | -0.00629 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -23.4    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 108      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | -0.0134  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -24.9    |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 155      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 651      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17      |\n",
      "|    explained_variance | 0.00515  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -218     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 749      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 652      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00343  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -296     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.69e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 653      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00601  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 601      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 5.72e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 653      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00158  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 120      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.88e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 652      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00291  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -891     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.98e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1163593.762953243 | Total rewards : 1143593.762953243 | % of profit : 5717.968814766215\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 647      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0365   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 77.5     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 46.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 647      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0357   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 67.7     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 73.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0153   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 116      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 350      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0311   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -42.9    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 130      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0214   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -328     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 878      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 650      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0474   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -97.4    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 351      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 650      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0123   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 343      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.55e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 651      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00513  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 379      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.66e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 553254.0998443189 | Total rewards : 533254.0998443189 | % of profit : 2666.2704992215945\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0176   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -29      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 57.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.124   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 44.7     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 113      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0603   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 29.1     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 150      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0161   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -93.8    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 469      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0254  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -128     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 351      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0169  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 208      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 262      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00241  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -207     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.34e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00422  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -285     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 7.46e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 457505.4111811831 | Total rewards : 437505.4111811831 | % of profit : 2187.527055905916\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0313  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 78.3     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 28.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 649      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00831  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 61.5     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 154      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0386   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -60.7    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 167      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.00167 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 156      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 325      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.00871 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 111      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 647      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 648      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00232  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 1.11e+03 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.04e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 646       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.2     |\n",
      "|    explained_variance | -0.000432 |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 149       |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.07e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 727424.8660791995 | Total rewards : 707424.8660791995 | % of profit : 3537.1243303959973\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 645      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.057   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -61.7    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 63.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 646      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.173    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -34.7    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 28.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 646      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0642   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -23      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 25.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 645      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0205   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -37.7    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 111      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 645      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.18     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -42      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 77.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 644      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0133   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 63.9     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 449      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 643      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.00501  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 906      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 5.87e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 642      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00522  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -410     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.51e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1348694.4355578574 | Total rewards : 1328694.4355578574 | % of profit : 6643.472177789287\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | -0.0295  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 164      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 106      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 641      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.0267   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 10.7     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 128      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 640      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.1    |\n",
      "|    explained_variance | 0.107    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 635      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.24e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0394   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -9.35    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 885      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 639      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | -0.0101  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -106     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 92.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0135   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 220      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 513      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 638      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.0231   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 706      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.41e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 637      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.00199  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 106      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.73e+03 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 639839.5466927632 | Total rewards : 619839.5466927632 | % of profit : 3099.197733463816\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 636      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.087    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -132     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 153      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 636      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0139  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -206     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 435      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0363  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 269      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 787      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0462   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -9.31    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0038   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -495     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.86e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 635      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00206  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 364      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 8.89e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 634       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.2     |\n",
      "|    explained_variance | 0.00774   |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -1.57e+03 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.12e+04  |\n",
      "-------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 772718.4256534586 | Total rewards : 752718.4256534586 | % of profit : 3763.5921282672925\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.2    |\n",
      "|    explained_variance | 0.143    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 263      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 791      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0276  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 8.94     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 97       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 634      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0665   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -67.6    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 114      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0251   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -214     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.05e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0392   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 3.59     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 760      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 633      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0163   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -335     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.39e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.00404 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -430     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.29e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00478  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 398      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.58e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 750636.8909874642 | Total rewards : 730636.8909874642 | % of profit : 3653.184454937321\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0485   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 16.9     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 19.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 632      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0125  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | -29.1    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 157      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 50       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0109   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -31.3    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 6.48e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.0106   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 561      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.45e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.135   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 138      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 171      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0115  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 500      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.49e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | -0.0063  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 1.72e+03 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.71e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.3    |\n",
      "|    explained_variance | 0.00965  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 997      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.18e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1431185.4415373309 | Total rewards : 1411185.4415373309 | % of profit : 7055.927207686654\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0981   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 59.6     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 17.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.112    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 18.7     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 20.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.125   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 58.4     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 58.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.0534  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | -96      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 85.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0208   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -547     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.63e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0271   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 529      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 3.43e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 631      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.00214  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 695      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.73e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.0371  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 258      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 993      |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 547873.854370985 | Total rewards : 527873.854370985 | % of profit : 2639.3692718549246\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0362   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 77.9     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 208      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0497   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 250      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 545      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.00957  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -135     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.00328  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 38.8     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.16e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 65       |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.0152   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -714     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 5.41e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 65       |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0025   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 7.84e+03 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 8.93e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 630       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.4     |\n",
      "|    explained_variance | 0.0134    |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -1.49e+03 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.36e+04  |\n",
      "-------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1641791.2313207095 | Total rewards : 1621791.2313207095 | % of profit : 8108.956156603548\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.147    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 69.1     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 40.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0332   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -75.7    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.042    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -290     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 848      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.1      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -36.9    |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 118      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.0128  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 51.6     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 205      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.142   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 406      |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 852      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0238   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -355     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 9.24e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.5    |\n",
      "|    explained_variance | 0.00841  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -435     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.69e+04 |\n",
      "------------------------------------\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 1351752.110210122 | Total rewards : 1331752.110210122 | % of profit : 6658.76055105061\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0973   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 16.1     |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 70.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0247   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -8.42    |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 818      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0673   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -13      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 551      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0354   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -42.5    |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 415      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.00798  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 1.34e+03 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 2.52e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 629      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 77       |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | 0.0211   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 433      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.27e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 630      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 78       |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -17.4    |\n",
      "|    explained_variance | -0.00671 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 1.4e+03  |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 1.03e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 630       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -17.4     |\n",
      "|    explained_variance | 0.00548   |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -1.66e+03 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.73e+04  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Modify Policy network\n",
    "\n",
    "import torch.nn as nn\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[\n",
    "      dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic\n",
    "    ],\n",
    "    activation_fn=nn.Tanh,\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 50000,\n",
    "    \"env_name\": train_env,\n",
    "}\n",
    "\n",
    "hyperparams = dict(\n",
    "    n_steps=5, # number of steps to collect data before updating policy\n",
    "    learning_rate=0.0005,\n",
    "    gamma=0.1, # discount factor\n",
    "    max_grad_norm=0.5, # The maximum value for the gradient clipping\n",
    "    ent_coef=0.05, # Entropy coefficient for the loss calculation\n",
    ")\n",
    "\"\"\"\n",
    "hyperparams = dict(\n",
    "    n_steps=7, # number of steps to collect data before updating policy\n",
    "    learning_rate=0.001003582768121682,\n",
    "    gamma=0.0018126757076625249, # discount factor\n",
    "    max_grad_norm=0.2819622027161751, # The maximum value for the gradient clipping\n",
    "    ent_coef=0.05, # Entropy coefficient for the loss calculation\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"20240421_US_STOCK_experiment\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "MF_P_Network_model = A2C(\"MlpPolicy\", train_env, seed=10, verbose=1, **hyperparams,tensorboard_log=f\"runs/{run.id}\").learn(50000,\n",
    "        callback=WandbCallback(\n",
    "        gradient_save_freq=100,\n",
    "        model_save_path=f\"wb_logs/{run.id}\",\n",
    "        verbose=2,\n",
    "\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf21ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n",
      "Initial Portfolio Value : 20000 | Final Portfolio Value : 70201.44942474365 | Total rewards : 50201.44942474365 | % of profit : 251.00724712371826\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(MF_P_Network_model, trade_env, n_eval_episodes=100, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3348c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\silve\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (23.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Requirement already satisfied: tqdm in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\silve\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\silve\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\silve\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Automatic Hyperparameter Tuning(Optuna)\n",
    "# In this part we will create a script that allows to search for the best hyperparameters automatically\n",
    "\n",
    "!pip install optuna\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 25  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(5e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 3 # Number of environments to evaluate the agent\n",
    "N_EVAL_EPISODES = 10 # Number of episodes to evaluate the agent\n",
    "TIMEOUT = int(60 * 60*6)  # Time budget in seconds\n",
    "\n",
    "#ENV_ID = \"StockTradingEnv-v0\"\n",
    "ENV_ID = train_env\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"seed\":10,\n",
    "    \"env\":ENV_ID,\n",
    "    \"tensorboard_log\":\"./logs\",\n",
    "}\n",
    "\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "import gymnasium\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for A2C hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.1, 2.0, log=True)\n",
    "    n_steps = trial.suggest_int(\"suggest_n_steps\", 5, 10 , step=1, log=True)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.01, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "    ent_coef= trial.suggest_float(\"ent_coef\", 0.001, 0.1, log=True)\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    #raise NotImplementedError(\"Implement sample_a2c_params()!\")\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]}\n",
    "        if net_arch == \"tiny\"\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "\n",
    "\n",
    "        },\n",
    "    }\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "\n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 1000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "#from wandb.keras import WandbCallback\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO:\n",
    "    # 1. Sample hyperparameters and update the default keyword arguments: `kwargs.update(other_params)`\n",
    "    # 2. Create the evaluation envs\n",
    "    # 3. Create the `TrialEvalCallback`\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    kwargs.update(sample_a2c_params(trial))\n",
    "    # Create the RL model\n",
    "    Optuna_Trial_model = A2C( **kwargs)\n",
    "\n",
    "    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
    "    eval_env=train_env\n",
    "    # eval_env = Monitor(gym.make(ENV_ID))\n",
    "    #eval_env = Monitor(train_env)\n",
    "    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
    "    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n",
    "    # TrialEvalCallback signature:\n",
    "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
    "\n",
    "\n",
    "\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True,verbose=1,\n",
    "    )\n",
    "\n",
    "    #WandbCallback=WandbCallback(gradient_save_freq=100,model_save_path=f\"wb_logs/{run.id}\",verbose=2,)\n",
    "\n",
    "    #callbacks=[WandbCallback,eval_callback]\n",
    "\n",
    "    config = {\n",
    "            \"policy_type\": \"MlpPolicy\",\n",
    "            #\"total_timesteps\": 25000,\n",
    "            \"env_name\": train_env,\n",
    "        }\n",
    "\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        #model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "        run = wandb.init(\n",
    "            project=\"20240421_US_STOCK_experiment\",\n",
    "            config=config,\n",
    "            sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "            monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "            save_code=True,  # optional\n",
    "        )\n",
    "        Optuna_Trial_model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "        Optuna_Trial_model.save(f\"wb_logs/{run.name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        Optuna_Trial_model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "\n",
    "    # Update best model if the current trial has higher reward\n",
    "    #global best_reward, best_model_params\n",
    "    #if mean_reward > best_reward:\n",
    "    #      best_reward = mean_reward\n",
    "    #      best_model_params = Optuna_Trial_model.get_parameters()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f03858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters Optimization Start!\n"
     ]
    }
   ],
   "source": [
    "print('Parameters Optimization Start!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "357d20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_optuma == 'yes':\n",
    "\n",
    "    import torch as th\n",
    "\n",
    "    # Set pytorch num threads to 1 for faster training\n",
    "    th.set_num_threads(1)\n",
    "    # Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used\n",
    "    #pruner = MedianPruner(\n",
    "    #    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    "    #)\n",
    "\n",
    "    pruner=optuna.pruners.HyperbandPruner(\n",
    "                                            min_resource=1,\n",
    "                                            max_resource=N_TRIALS,\n",
    "                                            reduction_factor=3\n",
    "        )\n",
    "\n",
    "    # Create the study and start the hyperparameter optimization\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Write report\n",
    "    study.trials_dataframe().to_csv(\"study_results_a2c_optuna.csv\")\n",
    "\n",
    "    fig1 = plot_optimization_history(study)\n",
    "    fig2 = plot_param_importances(study)\n",
    "\n",
    "    fig1.show()\n",
    "    fig2.show()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "971e8da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters Optimization Done!\n"
     ]
    }
   ],
   "source": [
    "print('Parameters Optimization Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed2555c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dashing-moon-735\n"
     ]
    }
   ],
   "source": [
    "# Look into WB logs and find the latest optimized run and get the run name\n",
    "import os\n",
    "CWD = os.getcwd()\n",
    "WB_Logs_Path = CWD + \"//wb_logs//\"\n",
    "WB_Logs_Dir_list = os.listdir(WB_Logs_Path)\n",
    "#print(WB_Logs_Dir_list)\n",
    "\n",
    "# Remove \".zip\" extension\n",
    "cleaned_list = [item.replace('.zip', '') for item in WB_Logs_Dir_list]\n",
    "\n",
    "# Sort the list based on the last 3 characters in descending order\n",
    "WB_Logs_Dir_Sorted_list = sorted(cleaned_list, key=lambda x: x[-3:], reverse=True)\n",
    "\n",
    "#print(' ')\n",
    "#print(WB_Logs_Dir_Sorted_list)\n",
    "\n",
    "import re\n",
    "\n",
    "# Regular expression pattern for \"something-something-number\"\n",
    "pattern = re.compile(r'^[a-z]+-[a-z]+-\\d{3}$')\n",
    "\n",
    "# Filter the list to include only items that match the pattern\n",
    "WB_Logs_Dir_Filtered_list = [item for item in WB_Logs_Dir_Sorted_list if pattern.match(item)]\n",
    "\n",
    "#print(' ')\n",
    "#print(WB_Logs_Dir_Filtered_list)\n",
    "\n",
    "# Grab the first item in the list\n",
    "A2C_Model_Latest_Run = WB_Logs_Dir_Filtered_list[0]\n",
    "\n",
    "#print(' ')\n",
    "print(A2C_Model_Latest_Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a686392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Best Model\n",
    "Best_A2C_Model = A2C_Model_Latest_Run\n",
    "Best_model=A2C.load(f\"wb_logs/{Best_A2C_Model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16246350",
   "metadata": {},
   "source": [
    "# Setup Trade Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cf73677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'AMZN', 'GOLD', 'GOOG', 'GOOGL', 'META', 'MSFT', 'SMCI', 'SNOW', 'TSLA', 'TSM', 'CRM']\n"
     ]
    }
   ],
   "source": [
    "# Check Stocks in Tic list\n",
    "print(tic)\n",
    "# tic = ['AAPL','AMZN','GOLD','GOOG','GOOGL','META','MSFT','SMCI','SNOW','TSLA','TSM']\n",
    "\n",
    "# Should be this:\n",
    "# tic = ['AAPL','AMZN','GOLD','GOOG','GOOGL','META','MSFT','SMCI','SNOW','TSLA','TSM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fbdca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 10, 270, 2, 40, 10, 10, 2, 20, 2, 70, 10]\n"
     ]
    }
   ],
   "source": [
    "# Stock holdings\n",
    "#Current_Tic_Holdings_list = [53,10,270,2,40,10,10,2,20,2,70]\n",
    "print(Current_Tic_Holdings_list)\n",
    "\n",
    "# Should look like this:\n",
    "# [53,10,270,2,40,10,10,2,20,2,70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da299a85",
   "metadata": {},
   "source": [
    "# Parameters 3 - Trade Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8036f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "ticLen = len(tic)               # number of stocks in tic list\n",
    "Shares_Per_Trade = 9999         # Maximum shares that can be purchased or sold in a trade\n",
    "#Initial_Investment = 5959.32    # Initial Investment\n",
    "Initial_Investment = 3768.32    # Initial Investment\n",
    "Action_Space = ticLen           # Number of stocks\n",
    "\n",
    "cd_cash = 12000                 # CD cash value\n",
    "Algorithmic_Trading_Result = [] # Output string for algorithmic trading\n",
    "\n",
    "# No. of Initial Investment + No. of Stocks Closing Prices + No. of Shares holding per stock + No. of stocks * No. of Technical Indicators\n",
    "\n",
    "No_Of_Initial_Investment = 1                             # No. of Initial Investment\n",
    "No_Of_Closing_prices = Action_Space                      # No. of Stocks Closing Prices\n",
    "No_Of_Shares_Per_Stock = len(Current_Tic_Holdings_list)  # No. of Shares held per stock\n",
    "No_Of_Stocks = Action_Space                              # No. of stocks\n",
    "No_Of_Technical_Indicators = 5                           # No. of Technical Indicators\n",
    "\n",
    "Observation_Space = No_Of_Initial_Investment + No_Of_Closing_prices + No_Of_Shares_Per_Stock + No_Of_Stocks * No_Of_Technical_Indicators\n",
    "                     \n",
    "# Ensure the calculation is correct\n",
    "assert Observation_Space == (1 + ticLen + len(Current_Tic_Holdings_list) + ticLen * No_Of_Technical_Indicators)\n",
    "\n",
    "duplicate_to_next_day = 'yes' # if you want to duplicate trading data to next trading day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57d649f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31\n"
     ]
    }
   ],
   "source": [
    "#Trading Data - 2 days before current date\n",
    "print(twoDaysBeforeTodays_date)\n",
    "inputStartDate = twoDaysBeforeTodays_date\n",
    "#inputStartDate = '2024-05-28'\n",
    "dateRangeList = []\n",
    "trading_data = stock_df[(stock_df.Date >= inputStartDate)]\n",
    "trading_data.sort_values([\"Date\", \"Ticker\"], ignore_index=True)\n",
    "trading_data.index = trading_data.Date.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4376b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_market_calendars in c:\\users\\silve\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas_market_calendars) (1.5.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas_market_calendars) (2022.7)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas_market_calendars) (2.8.2)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from pandas_market_calendars) (4.5.4)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.24.3)\n",
      "Requirement already satisfied: pyluach in c:\\users\\silve\\anaconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\silve\\anaconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\silve\\anaconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2024.1)\n",
      "Requirement already satisfied: korean-lunar-calendar in c:\\users\\silve\\anaconda3\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silve\\anaconda3\\lib\\site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_market_calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da4b0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if duplicate_to_next_day == 'yes':\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pandas_market_calendars as mcal\n",
    "\n",
    "    latest_trading_date = trading_data['Date'].iloc[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    # duplicate the latest day to the next day\n",
    "    # and see whether if predicts\n",
    "\n",
    "    latest_trading_date_df = trading_data[trading_data['Date']== latest_trading_date]\n",
    "    latest_trading_date_df\n",
    "\n",
    "    # Function to get the next trading day\n",
    "    def get_next_trading_day(current_date):\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        schedule = nyse.schedule(start_date=current_date, end_date=current_date + pd.DateOffset(days=10))\n",
    "        next_trading_day = schedule.iloc[1].name  # get the next trading day after the given date\n",
    "        return next_trading_day\n",
    "\n",
    "    # Duplicate the DataFrame\n",
    "    new_rows = latest_trading_date_df.copy()\n",
    "\n",
    "    # Convert Date column to datetime\n",
    "    latest_trading_date_df['Date'] = pd.to_datetime(latest_trading_date_df['Date'])\n",
    "    new_rows['Date'] = latest_trading_date_df['Date'].apply(get_next_trading_day)\n",
    "\n",
    "    # Combine the original and new DataFrames\n",
    "    combined_df = pd.concat([latest_trading_date_df, new_rows])\n",
    "\n",
    "    # Reset the index\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    trading_data = combined_df\n",
    "    trading_data.sort_values([\"Date\", \"Ticker\"], ignore_index=True)\n",
    "    trading_data.index = trading_data.Date.factorize()[0]\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b48c457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_ppo</th>\n",
       "      <th>momentum_ppo_signal</th>\n",
       "      <th>momentum_ppo_hist</th>\n",
       "      <th>momentum_pvo</th>\n",
       "      <th>momentum_pvo_signal</th>\n",
       "      <th>momentum_pvo_hist</th>\n",
       "      <th>momentum_kama</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>others_dlr</th>\n",
       "      <th>others_cr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>191.440002</td>\n",
       "      <td>192.570007</td>\n",
       "      <td>189.910004</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>74776921.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.279608e+13</td>\n",
       "      <td>3.644316e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.299492</td>\n",
       "      <td>2.390772</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>-4.263600</td>\n",
       "      <td>-6.941010</td>\n",
       "      <td>2.677410</td>\n",
       "      <td>189.520314</td>\n",
       "      <td>0.501859</td>\n",
       "      <td>0.500604</td>\n",
       "      <td>6915.981339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>178.229996</td>\n",
       "      <td>179.210007</td>\n",
       "      <td>173.869995</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>58350991.0</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2.218243e+10</td>\n",
       "      <td>1.794569e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.392337</td>\n",
       "      <td>0.153654</td>\n",
       "      <td>-0.545991</td>\n",
       "      <td>-3.457333</td>\n",
       "      <td>-5.138151</td>\n",
       "      <td>1.680818</td>\n",
       "      <td>180.963708</td>\n",
       "      <td>-1.606070</td>\n",
       "      <td>-1.619107</td>\n",
       "      <td>6391.538140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>16.969999</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>15459464.0</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>-4.957437e+11</td>\n",
       "      <td>1.169816e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484825</td>\n",
       "      <td>0.793227</td>\n",
       "      <td>-0.308402</td>\n",
       "      <td>-11.289881</td>\n",
       "      <td>-10.420045</td>\n",
       "      <td>-0.869836</td>\n",
       "      <td>17.607786</td>\n",
       "      <td>-0.116893</td>\n",
       "      <td>-0.116962</td>\n",
       "      <td>-38.894905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>173.192001</td>\n",
       "      <td>174.419998</td>\n",
       "      <td>170.970001</td>\n",
       "      <td>173.960007</td>\n",
       "      <td>173.960007</td>\n",
       "      <td>27173303.0</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>7.315997e+09</td>\n",
       "      <td>9.383309e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.030185</td>\n",
       "      <td>2.545757</td>\n",
       "      <td>-0.515571</td>\n",
       "      <td>-3.671716</td>\n",
       "      <td>-7.876583</td>\n",
       "      <td>4.204867</td>\n",
       "      <td>174.166383</td>\n",
       "      <td>0.230473</td>\n",
       "      <td>0.230208</td>\n",
       "      <td>2073.684105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>171.860001</td>\n",
       "      <td>173.059998</td>\n",
       "      <td>169.440002</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>36174235.0</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>7.752538e+09</td>\n",
       "      <td>1.148508e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063114</td>\n",
       "      <td>2.574123</td>\n",
       "      <td>-0.511008</td>\n",
       "      <td>-4.306067</td>\n",
       "      <td>-7.978229</td>\n",
       "      <td>3.672162</td>\n",
       "      <td>172.725041</td>\n",
       "      <td>0.226599</td>\n",
       "      <td>0.226343</td>\n",
       "      <td>2045.244527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>465.799988</td>\n",
       "      <td>469.119995</td>\n",
       "      <td>454.460114</td>\n",
       "      <td>466.829987</td>\n",
       "      <td>466.829987</td>\n",
       "      <td>16645191.0</td>\n",
       "      <td>META</td>\n",
       "      <td>-4.677736e+11</td>\n",
       "      <td>4.938520e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293233</td>\n",
       "      <td>-0.460612</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>-11.198798</td>\n",
       "      <td>-12.709827</td>\n",
       "      <td>1.511029</td>\n",
       "      <td>470.257292</td>\n",
       "      <td>-0.047104</td>\n",
       "      <td>-0.047116</td>\n",
       "      <td>1122.404674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>416.410004</td>\n",
       "      <td>416.630005</td>\n",
       "      <td>404.519989</td>\n",
       "      <td>415.130005</td>\n",
       "      <td>415.130005</td>\n",
       "      <td>47676297.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-2.362908e+12</td>\n",
       "      <td>4.909677e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.915615</td>\n",
       "      <td>-0.121235</td>\n",
       "      <td>10.867785</td>\n",
       "      <td>-1.274575</td>\n",
       "      <td>12.142360</td>\n",
       "      <td>424.664985</td>\n",
       "      <td>0.110930</td>\n",
       "      <td>0.110868</td>\n",
       "      <td>2665.386797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>815.770020</td>\n",
       "      <td>819.650024</td>\n",
       "      <td>760.010010</td>\n",
       "      <td>784.510010</td>\n",
       "      <td>784.510010</td>\n",
       "      <td>7168478.0</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>1.488473e+08</td>\n",
       "      <td>2.845687e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773555</td>\n",
       "      <td>-0.276110</td>\n",
       "      <td>-0.497445</td>\n",
       "      <td>-1.142673</td>\n",
       "      <td>-2.144296</td>\n",
       "      <td>1.001623</td>\n",
       "      <td>855.496104</td>\n",
       "      <td>-5.245548</td>\n",
       "      <td>-5.388136</td>\n",
       "      <td>12138.845998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>140.270004</td>\n",
       "      <td>142.100006</td>\n",
       "      <td>133.589996</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>14004884.0</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>-5.102560e+10</td>\n",
       "      <td>1.063237e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.332818</td>\n",
       "      <td>-0.674064</td>\n",
       "      <td>-1.658754</td>\n",
       "      <td>23.442811</td>\n",
       "      <td>11.950284</td>\n",
       "      <td>11.492528</td>\n",
       "      <td>149.282159</td>\n",
       "      <td>-3.384182</td>\n",
       "      <td>-3.442771</td>\n",
       "      <td>-46.371048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>178.419998</td>\n",
       "      <td>180.320007</td>\n",
       "      <td>173.820099</td>\n",
       "      <td>178.080002</td>\n",
       "      <td>178.080002</td>\n",
       "      <td>67110156.0</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>8.771155e+10</td>\n",
       "      <td>1.250517e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899642</td>\n",
       "      <td>0.955808</td>\n",
       "      <td>-0.056166</td>\n",
       "      <td>-10.414795</td>\n",
       "      <td>-9.378482</td>\n",
       "      <td>-1.036313</td>\n",
       "      <td>172.862631</td>\n",
       "      <td>-0.397109</td>\n",
       "      <td>-0.397900</td>\n",
       "      <td>11081.245274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>151.479996</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>147.809998</td>\n",
       "      <td>151.039993</td>\n",
       "      <td>151.039993</td>\n",
       "      <td>13738764.0</td>\n",
       "      <td>TSM</td>\n",
       "      <td>-9.220333e+11</td>\n",
       "      <td>2.125963e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.636883</td>\n",
       "      <td>2.898447</td>\n",
       "      <td>-0.261565</td>\n",
       "      <td>-1.417396</td>\n",
       "      <td>-3.515156</td>\n",
       "      <td>2.097760</td>\n",
       "      <td>152.550118</td>\n",
       "      <td>-1.255239</td>\n",
       "      <td>-1.263184</td>\n",
       "      <td>2918.881568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>219.110001</td>\n",
       "      <td>234.619995</td>\n",
       "      <td>216.093002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>35984705.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.159477e+09</td>\n",
       "      <td>9.529935e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.244339</td>\n",
       "      <td>-1.364971</td>\n",
       "      <td>-1.879368</td>\n",
       "      <td>58.297647</td>\n",
       "      <td>21.284997</td>\n",
       "      <td>37.012649</td>\n",
       "      <td>254.292464</td>\n",
       "      <td>7.536355</td>\n",
       "      <td>7.265879</td>\n",
       "      <td>2660.096806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>191.440002</td>\n",
       "      <td>192.570007</td>\n",
       "      <td>189.910004</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>192.250000</td>\n",
       "      <td>74776921.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.279608e+13</td>\n",
       "      <td>3.644316e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.299492</td>\n",
       "      <td>2.390772</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>-4.263600</td>\n",
       "      <td>-6.941010</td>\n",
       "      <td>2.677410</td>\n",
       "      <td>189.520314</td>\n",
       "      <td>0.501859</td>\n",
       "      <td>0.500604</td>\n",
       "      <td>6915.981339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>178.229996</td>\n",
       "      <td>179.210007</td>\n",
       "      <td>173.869995</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>58350991.0</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2.218243e+10</td>\n",
       "      <td>1.794569e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.392337</td>\n",
       "      <td>0.153654</td>\n",
       "      <td>-0.545991</td>\n",
       "      <td>-3.457333</td>\n",
       "      <td>-5.138151</td>\n",
       "      <td>1.680818</td>\n",
       "      <td>180.963708</td>\n",
       "      <td>-1.606070</td>\n",
       "      <td>-1.619107</td>\n",
       "      <td>6391.538140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>17.129999</td>\n",
       "      <td>17.260000</td>\n",
       "      <td>16.969999</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>17.090000</td>\n",
       "      <td>15459464.0</td>\n",
       "      <td>GOLD</td>\n",
       "      <td>-4.957437e+11</td>\n",
       "      <td>1.169816e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484825</td>\n",
       "      <td>0.793227</td>\n",
       "      <td>-0.308402</td>\n",
       "      <td>-11.289881</td>\n",
       "      <td>-10.420045</td>\n",
       "      <td>-0.869836</td>\n",
       "      <td>17.607786</td>\n",
       "      <td>-0.116893</td>\n",
       "      <td>-0.116962</td>\n",
       "      <td>-38.894905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>173.192001</td>\n",
       "      <td>174.419998</td>\n",
       "      <td>170.970001</td>\n",
       "      <td>173.960007</td>\n",
       "      <td>173.960007</td>\n",
       "      <td>27173303.0</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>7.315997e+09</td>\n",
       "      <td>9.383309e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.030185</td>\n",
       "      <td>2.545757</td>\n",
       "      <td>-0.515571</td>\n",
       "      <td>-3.671716</td>\n",
       "      <td>-7.876583</td>\n",
       "      <td>4.204867</td>\n",
       "      <td>174.166383</td>\n",
       "      <td>0.230473</td>\n",
       "      <td>0.230208</td>\n",
       "      <td>2073.684105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>171.860001</td>\n",
       "      <td>173.059998</td>\n",
       "      <td>169.440002</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>36174235.0</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>7.752538e+09</td>\n",
       "      <td>1.148508e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063114</td>\n",
       "      <td>2.574123</td>\n",
       "      <td>-0.511008</td>\n",
       "      <td>-4.306067</td>\n",
       "      <td>-7.978229</td>\n",
       "      <td>3.672162</td>\n",
       "      <td>172.725041</td>\n",
       "      <td>0.226599</td>\n",
       "      <td>0.226343</td>\n",
       "      <td>2045.244527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>465.799988</td>\n",
       "      <td>469.119995</td>\n",
       "      <td>454.460114</td>\n",
       "      <td>466.829987</td>\n",
       "      <td>466.829987</td>\n",
       "      <td>16645191.0</td>\n",
       "      <td>META</td>\n",
       "      <td>-4.677736e+11</td>\n",
       "      <td>4.938520e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293233</td>\n",
       "      <td>-0.460612</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>-11.198798</td>\n",
       "      <td>-12.709827</td>\n",
       "      <td>1.511029</td>\n",
       "      <td>470.257292</td>\n",
       "      <td>-0.047104</td>\n",
       "      <td>-0.047116</td>\n",
       "      <td>1122.404674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>416.410004</td>\n",
       "      <td>416.630005</td>\n",
       "      <td>404.519989</td>\n",
       "      <td>415.130005</td>\n",
       "      <td>415.130005</td>\n",
       "      <td>47676297.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>-2.362908e+12</td>\n",
       "      <td>4.909677e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794380</td>\n",
       "      <td>0.915615</td>\n",
       "      <td>-0.121235</td>\n",
       "      <td>10.867785</td>\n",
       "      <td>-1.274575</td>\n",
       "      <td>12.142360</td>\n",
       "      <td>424.664985</td>\n",
       "      <td>0.110930</td>\n",
       "      <td>0.110868</td>\n",
       "      <td>2665.386797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>815.770020</td>\n",
       "      <td>819.650024</td>\n",
       "      <td>760.010010</td>\n",
       "      <td>784.510010</td>\n",
       "      <td>784.510010</td>\n",
       "      <td>7168478.0</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>1.488473e+08</td>\n",
       "      <td>2.845687e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773555</td>\n",
       "      <td>-0.276110</td>\n",
       "      <td>-0.497445</td>\n",
       "      <td>-1.142673</td>\n",
       "      <td>-2.144296</td>\n",
       "      <td>1.001623</td>\n",
       "      <td>855.496104</td>\n",
       "      <td>-5.245548</td>\n",
       "      <td>-5.388136</td>\n",
       "      <td>12138.845998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>140.270004</td>\n",
       "      <td>142.100006</td>\n",
       "      <td>133.589996</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>14004884.0</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>-5.102560e+10</td>\n",
       "      <td>1.063237e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.332818</td>\n",
       "      <td>-0.674064</td>\n",
       "      <td>-1.658754</td>\n",
       "      <td>23.442811</td>\n",
       "      <td>11.950284</td>\n",
       "      <td>11.492528</td>\n",
       "      <td>149.282159</td>\n",
       "      <td>-3.384182</td>\n",
       "      <td>-3.442771</td>\n",
       "      <td>-46.371048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>178.419998</td>\n",
       "      <td>180.320007</td>\n",
       "      <td>173.820099</td>\n",
       "      <td>178.080002</td>\n",
       "      <td>178.080002</td>\n",
       "      <td>67110156.0</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>8.771155e+10</td>\n",
       "      <td>1.250517e+11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899642</td>\n",
       "      <td>0.955808</td>\n",
       "      <td>-0.056166</td>\n",
       "      <td>-10.414795</td>\n",
       "      <td>-9.378482</td>\n",
       "      <td>-1.036313</td>\n",
       "      <td>172.862631</td>\n",
       "      <td>-0.397109</td>\n",
       "      <td>-0.397900</td>\n",
       "      <td>11081.245274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>151.479996</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>147.809998</td>\n",
       "      <td>151.039993</td>\n",
       "      <td>151.039993</td>\n",
       "      <td>13738764.0</td>\n",
       "      <td>TSM</td>\n",
       "      <td>-9.220333e+11</td>\n",
       "      <td>2.125963e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.636883</td>\n",
       "      <td>2.898447</td>\n",
       "      <td>-0.261565</td>\n",
       "      <td>-1.417396</td>\n",
       "      <td>-3.515156</td>\n",
       "      <td>2.097760</td>\n",
       "      <td>152.550118</td>\n",
       "      <td>-1.255239</td>\n",
       "      <td>-1.263184</td>\n",
       "      <td>2918.881568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>219.110001</td>\n",
       "      <td>234.619995</td>\n",
       "      <td>216.093002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>234.440002</td>\n",
       "      <td>35984705.0</td>\n",
       "      <td>CRM</td>\n",
       "      <td>-1.159477e+09</td>\n",
       "      <td>9.529935e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.244339</td>\n",
       "      <td>-1.364971</td>\n",
       "      <td>-1.879368</td>\n",
       "      <td>58.297647</td>\n",
       "      <td>21.284997</td>\n",
       "      <td>37.012649</td>\n",
       "      <td>254.292464</td>\n",
       "      <td>7.536355</td>\n",
       "      <td>7.265879</td>\n",
       "      <td>2660.096806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date        Open        High         Low       Close    AdjClose  \\\n",
       "0 2024-05-31  191.440002  192.570007  189.910004  192.250000  192.250000   \n",
       "0 2024-05-31  178.229996  179.210007  173.869995  176.440002  176.440002   \n",
       "0 2024-05-31   17.129999   17.260000   16.969999   17.090000   17.090000   \n",
       "0 2024-05-31  173.192001  174.419998  170.970001  173.960007  173.960007   \n",
       "0 2024-05-31  171.860001  173.059998  169.440002  172.500000  172.500000   \n",
       "0 2024-05-31  465.799988  469.119995  454.460114  466.829987  466.829987   \n",
       "0 2024-05-31  416.410004  416.630005  404.519989  415.130005  415.130005   \n",
       "0 2024-05-31  815.770020  819.650024  760.010010  784.510010  784.510010   \n",
       "0 2024-05-31  140.270004  142.100006  133.589996  136.179993  136.179993   \n",
       "0 2024-05-31  178.419998  180.320007  173.820099  178.080002  178.080002   \n",
       "0 2024-05-31  151.479996  152.500000  147.809998  151.039993  151.039993   \n",
       "0 2024-05-31  219.110001  234.619995  216.093002  234.440002  234.440002   \n",
       "1 2024-06-03  191.440002  192.570007  189.910004  192.250000  192.250000   \n",
       "1 2024-06-03  178.229996  179.210007  173.869995  176.440002  176.440002   \n",
       "1 2024-06-03   17.129999   17.260000   16.969999   17.090000   17.090000   \n",
       "1 2024-06-03  173.192001  174.419998  170.970001  173.960007  173.960007   \n",
       "1 2024-06-03  171.860001  173.059998  169.440002  172.500000  172.500000   \n",
       "1 2024-06-03  465.799988  469.119995  454.460114  466.829987  466.829987   \n",
       "1 2024-06-03  416.410004  416.630005  404.519989  415.130005  415.130005   \n",
       "1 2024-06-03  815.770020  819.650024  760.010010  784.510010  784.510010   \n",
       "1 2024-06-03  140.270004  142.100006  133.589996  136.179993  136.179993   \n",
       "1 2024-06-03  178.419998  180.320007  173.820099  178.080002  178.080002   \n",
       "1 2024-06-03  151.479996  152.500000  147.809998  151.039993  151.039993   \n",
       "1 2024-06-03  219.110001  234.619995  216.093002  234.440002  234.440002   \n",
       "\n",
       "       Volume Ticker    volume_adi    volume_obv  ...  momentum_ppo  \\\n",
       "0  74776921.0   AAPL -1.279608e+13  3.644316e+10  ...      2.299492   \n",
       "0  58350991.0   AMZN  2.218243e+10  1.794569e+10  ...     -0.392337   \n",
       "0  15459464.0   GOLD -4.957437e+11  1.169816e+09  ...      0.484825   \n",
       "0  27173303.0   GOOG  7.315997e+09  9.383309e+09  ...      2.030185   \n",
       "0  36174235.0  GOOGL  7.752538e+09  1.148508e+10  ...      2.063114   \n",
       "0  16645191.0   META -4.677736e+11  4.938520e+11  ...     -0.293233   \n",
       "0  47676297.0   MSFT -2.362908e+12  4.909677e+09  ...      0.794380   \n",
       "0   7168478.0   SMCI  1.488473e+08  2.845687e+08  ...     -0.773555   \n",
       "0  14004884.0   SNOW -5.102560e+10  1.063237e+11  ...     -2.332818   \n",
       "0  67110156.0   TSLA  8.771155e+10  1.250517e+11  ...      0.899642   \n",
       "0  13738764.0    TSM -9.220333e+11  2.125963e+09  ...      2.636883   \n",
       "0  35984705.0    CRM -1.159477e+09  9.529935e+08  ...     -3.244339   \n",
       "1  74776921.0   AAPL -1.279608e+13  3.644316e+10  ...      2.299492   \n",
       "1  58350991.0   AMZN  2.218243e+10  1.794569e+10  ...     -0.392337   \n",
       "1  15459464.0   GOLD -4.957437e+11  1.169816e+09  ...      0.484825   \n",
       "1  27173303.0   GOOG  7.315997e+09  9.383309e+09  ...      2.030185   \n",
       "1  36174235.0  GOOGL  7.752538e+09  1.148508e+10  ...      2.063114   \n",
       "1  16645191.0   META -4.677736e+11  4.938520e+11  ...     -0.293233   \n",
       "1  47676297.0   MSFT -2.362908e+12  4.909677e+09  ...      0.794380   \n",
       "1   7168478.0   SMCI  1.488473e+08  2.845687e+08  ...     -0.773555   \n",
       "1  14004884.0   SNOW -5.102560e+10  1.063237e+11  ...     -2.332818   \n",
       "1  67110156.0   TSLA  8.771155e+10  1.250517e+11  ...      0.899642   \n",
       "1  13738764.0    TSM -9.220333e+11  2.125963e+09  ...      2.636883   \n",
       "1  35984705.0    CRM -1.159477e+09  9.529935e+08  ...     -3.244339   \n",
       "\n",
       "   momentum_ppo_signal  momentum_ppo_hist  momentum_pvo  momentum_pvo_signal  \\\n",
       "0             2.390772          -0.091280     -4.263600            -6.941010   \n",
       "0             0.153654          -0.545991     -3.457333            -5.138151   \n",
       "0             0.793227          -0.308402    -11.289881           -10.420045   \n",
       "0             2.545757          -0.515571     -3.671716            -7.876583   \n",
       "0             2.574123          -0.511008     -4.306067            -7.978229   \n",
       "0            -0.460612           0.167378    -11.198798           -12.709827   \n",
       "0             0.915615          -0.121235     10.867785            -1.274575   \n",
       "0            -0.276110          -0.497445     -1.142673            -2.144296   \n",
       "0            -0.674064          -1.658754     23.442811            11.950284   \n",
       "0             0.955808          -0.056166    -10.414795            -9.378482   \n",
       "0             2.898447          -0.261565     -1.417396            -3.515156   \n",
       "0            -1.364971          -1.879368     58.297647            21.284997   \n",
       "1             2.390772          -0.091280     -4.263600            -6.941010   \n",
       "1             0.153654          -0.545991     -3.457333            -5.138151   \n",
       "1             0.793227          -0.308402    -11.289881           -10.420045   \n",
       "1             2.545757          -0.515571     -3.671716            -7.876583   \n",
       "1             2.574123          -0.511008     -4.306067            -7.978229   \n",
       "1            -0.460612           0.167378    -11.198798           -12.709827   \n",
       "1             0.915615          -0.121235     10.867785            -1.274575   \n",
       "1            -0.276110          -0.497445     -1.142673            -2.144296   \n",
       "1            -0.674064          -1.658754     23.442811            11.950284   \n",
       "1             0.955808          -0.056166    -10.414795            -9.378482   \n",
       "1             2.898447          -0.261565     -1.417396            -3.515156   \n",
       "1            -1.364971          -1.879368     58.297647            21.284997   \n",
       "\n",
       "   momentum_pvo_hist  momentum_kama  others_dr  others_dlr     others_cr  \n",
       "0           2.677410     189.520314   0.501859    0.500604   6915.981339  \n",
       "0           1.680818     180.963708  -1.606070   -1.619107   6391.538140  \n",
       "0          -0.869836      17.607786  -0.116893   -0.116962    -38.894905  \n",
       "0           4.204867     174.166383   0.230473    0.230208   2073.684105  \n",
       "0           3.672162     172.725041   0.226599    0.226343   2045.244527  \n",
       "0           1.511029     470.257292  -0.047104   -0.047116   1122.404674  \n",
       "0          12.142360     424.664985   0.110930    0.110868   2665.386797  \n",
       "0           1.001623     855.496104  -5.245548   -5.388136  12138.845998  \n",
       "0          11.492528     149.282159  -3.384182   -3.442771    -46.371048  \n",
       "0          -1.036313     172.862631  -0.397109   -0.397900  11081.245274  \n",
       "0           2.097760     152.550118  -1.255239   -1.263184   2918.881568  \n",
       "0          37.012649     254.292464   7.536355    7.265879   2660.096806  \n",
       "1           2.677410     189.520314   0.501859    0.500604   6915.981339  \n",
       "1           1.680818     180.963708  -1.606070   -1.619107   6391.538140  \n",
       "1          -0.869836      17.607786  -0.116893   -0.116962    -38.894905  \n",
       "1           4.204867     174.166383   0.230473    0.230208   2073.684105  \n",
       "1           3.672162     172.725041   0.226599    0.226343   2045.244527  \n",
       "1           1.511029     470.257292  -0.047104   -0.047116   1122.404674  \n",
       "1          12.142360     424.664985   0.110930    0.110868   2665.386797  \n",
       "1           1.001623     855.496104  -5.245548   -5.388136  12138.845998  \n",
       "1          11.492528     149.282159  -3.384182   -3.442771    -46.371048  \n",
       "1          -1.036313     172.862631  -0.397109   -0.397900  11081.245274  \n",
       "1           2.097760     152.550118  -1.255239   -1.263184   2918.881568  \n",
       "1          37.012649     254.292464   7.536355    7.265879   2660.096806  \n",
       "\n",
       "[24 rows x 94 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94157d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Stock Tracing Env 2 ****\n",
    "\n",
    "class StockTradingEnv2(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, Shares_Per_Trade = 10, Initial_Investment = 10000, Action_Space = ticLen, Observation_Space = 78, day = 0,\n",
    "                 Normalized_Rewards = 1e-4, verbosity = 0, mode = 'train', seed = 10, commission = 0  ):\n",
    "\n",
    "      self.day = day\n",
    "      self.df = df\n",
    "      self.max_shares_per_trade = Shares_Per_Trade\n",
    "      self.initial_investment = Initial_Investment\n",
    "      self.Action_Space = Action_Space\n",
    "      self.Observation_Space = Observation_Space\n",
    "      self.normalized_rewards = Normalized_Rewards\n",
    "      self.verbosity = verbosity\n",
    "      self.commission = commission\n",
    "      self.mode = mode\n",
    "      self._seed(seed)\n",
    "\n",
    "      #************************\n",
    "      self.ticker = df.Ticker.unique()\n",
    "\n",
    "\n",
    "      #Action Space\n",
    "      # Action > 0  means buy shares of stock\n",
    "      # Action 0 means Hold the stock\n",
    "      # Action < 0 means sell shares of stock\n",
    "      self.action_space = spaces.Box(low = -1, high= 1,\n",
    "                                    shape=(self.Action_Space,), dtype= int)\n",
    "      #Observation Space\n",
    "      self.observation_space = spaces.Box(low = -np.inf, high= np.inf, shape=(self.Observation_Space,))\n",
    "      #Selecting the Data for one date\n",
    "      self.data = self.df.loc[self.day,:]\n",
    "      #Initial Run\n",
    "      self.initial = True\n",
    "      #Verify if tradings days are completed or not\n",
    "      self.done = False\n",
    "      #Rewards\n",
    "      self.reward = 0\n",
    "      #Asset value after each trading day\n",
    "      self.asset_memory = [self.initial_investment]\n",
    "      #Rewards received for each trading day i.e profit or loss\n",
    "      self.reward_memory = []\n",
    "      #Saving the date for the trade\n",
    "      self.date_memory = [self.data.Date.unique()[0]]\n",
    "      #Initializing state of the environment\n",
    "      self.state = [self.initial_investment] + self.data.AdjClose.values.tolist() + Current_Tic_Holdings_list + \\\n",
    "        self.data.momentum_ppo.values.tolist() + self.data.momentum_rsi.values.tolist() + \\\n",
    "        self.data.trend_adx.values.tolist() + self.data.trend_macd.values.tolist() + \\\n",
    "        self.data.trend_cci.values.tolist()\n",
    "        \n",
    "      self.starting_portfolio_value = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                   np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "\n",
    "      # Ensure the length matches the observation space\n",
    "      assert len(self.state) == self.Observation_Space\n",
    "\n",
    "      #************************\n",
    "      self.keep_row_info = {}\n",
    "      self.keep_rows_list = []\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "      return self.state\n",
    "\n",
    "    # This method is used to reset the values of the state to it's default after every episode\n",
    "    def reset(self):\n",
    "      #print(\" \")\n",
    "      #print(\"reseting self...\")\n",
    "\n",
    "      self.day = 0\n",
    "      self.reward = 0\n",
    "      self.data = self.df.loc[self.day,:]\n",
    "      self.done = False\n",
    "      self.initial = False\n",
    "      self.reward_memory = []\n",
    "      self.date_memory = [str(self.data.Date.unique()[0])]\n",
    "      self.asset_memory = [self.initial_investment]\n",
    "      #***Technical Indicators tolist (check the  observation_space num !!!)\n",
    "      #***Search \"self.state =\"\n",
    "      self.state = [self.initial_investment] + self.data.AdjClose.values.tolist() + Current_Tic_Holdings_list + \\\n",
    "        self.data.momentum_ppo.values.tolist() + self.data.momentum_rsi.values.tolist() + \\\n",
    "        self.data.trend_adx.values.tolist() + self.data.trend_macd.values.tolist() + \\\n",
    "        self.data.trend_cci.values.tolist()\n",
    "\n",
    "      self.starting_portfolio_value = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                   np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "    \n",
    "      #************************\n",
    "      self.keep_row_info = {}\n",
    "      self.keep_rows_list = []\n",
    "      #print(self.state)\n",
    "      return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "      #************************\n",
    "      print(' ')\n",
    "      print('Day: ' + str(self.day))\n",
    "      dateRangeList.append(str(self.data.Date.unique()[0])[:10])\n",
    "      inputEndDate = str(self.data.Date.unique()[0])[:10]\n",
    "      print(str(self.data.Date.unique()[0])[:10])\n",
    "      print('Available cash before trading: ' + str(self.state[0]))\n",
    "\n",
    "      self.done = self.day >= len(self.df.Date.unique())-1\n",
    "      #Use this to save the results to csv after we performed trading for all the days\n",
    "      if self.done:\n",
    "\n",
    "        keep_df = pd.DataFrame(index = [x for x in range(len(self.keep_rows_list))],data = self.keep_rows_list)\n",
    "\n",
    "        #print('Keep DF')\n",
    "        #print(keep_df)\n",
    "\n",
    "        final_portfolio_value = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "        \n",
    "        final_final_portfolio_value = final_portfolio_value + 12000\n",
    "        starting_portfolio_value = self.starting_portfolio_value + 12000\n",
    "        \n",
    "        #total_rewards = final_portfolio_value - self.initial_investment\n",
    "        total_rewards = final_final_portfolio_value - starting_portfolio_value\n",
    "        profit_pct = (total_rewards*100)/starting_portfolio_value\n",
    "        asset_df = pd.DataFrame(self.asset_memory)\n",
    "        asset_df.columns = ['portfolio']\n",
    "        asset_df['date'] = self.date_memory\n",
    "        #if self.verbosity and self.mode != 'train':\n",
    "        if self.verbosity :\n",
    "          #print(len(self.reward_memory))\n",
    "          #if self.mode == 'trade' or self.mode == 'val':\n",
    "\n",
    "          asset_df['date'] = self.date_memory\n",
    "          print(' ')\n",
    "          Algorithmic_Trading_Result.append(str('Start Date : '+str(dateRangeList[0])+' | End Date : '+str(dateRangeList[-1])+' | Initial Portfolio Value : '+str(starting_portfolio_value)+' | Final Portfolio Value : '+str(final_final_portfolio_value)+' | Total rewards : '+str(total_rewards)+' | % of profit : '+str(profit_pct)))  \n",
    "          print('Start Date : {} | End Date : {} | Initial Portfolio Value : {} | Final Portfolio Value : {} | Total rewards : {} | % of profit : {}'.format(dateRangeList[0], dateRangeList[-1], starting_portfolio_value, final_final_portfolio_value, total_rewards, profit_pct))\n",
    "          asset_df.to_csv('{}_{}_results.csv'.format(self.mode, self.commission))\n",
    "\n",
    "        #************************\n",
    "        #keep_df = pd.DataFrame(keep_day_list, columns=['date'], index = [x for x in range(len(keep_day_list))])\n",
    "        #keep_df['portfolio_before_trade'] = keep_portfolio_before_trade_list\n",
    "        #keep_df['available_cash_after_trading'] = keep_available_cash_after_trading_list\n",
    "        #keep_df['shares_available_per_stock_list'] = keep_shares_available_per_stock_list\n",
    "        #keep_df['profit_and_loss_for_day'] = keep_profit_and_loss_for_day\n",
    "\n",
    "        #print(keep_df)\n",
    "\n",
    "        #keep_df.to_csv('keep_results.csv')\n",
    "\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "      else:\n",
    "\n",
    "        #Calculating the portfolio value before start of trading\n",
    "        #Available investment amount + sum of value of each stock held (no.of shares per stock * price of the stock on that day)\n",
    "        portfolio_before_trade = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "\n",
    "        print('CD value: ' + str(12000))\n",
    "        print('Stocks portfolio value before trading: ' + str(portfolio_before_trade))\n",
    "        print('Total portfolio value before trading: ' + str(portfolio_before_trade + 12000))\n",
    "        \n",
    "        #print stocks and available shares\n",
    "        print(' ')\n",
    "        print('** Stocks and holdings before trade: **')\n",
    "        for tidx in range(len(tic)):\n",
    "            ticker_symbol = self.ticker[tidx]\n",
    "            ticker_holdings = str(self.state[tidx+self.Action_Space+1])\n",
    "            print(ticker_symbol + ': ' + ticker_holdings + ' Shares')\n",
    "            \n",
    "        print(' ')\n",
    "        print('** Stocks Technical Indicators: **')\n",
    "        for tidx in range(len(tic)):\n",
    "            ticker_symbol = self.ticker[tidx]\n",
    "            print(ticker_symbol)\n",
    "            \n",
    "            print(self.data[self.data['Ticker'] == ticker_symbol][['momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci', 'AdjClose']])\n",
    "            print(' ')\n",
    "            \n",
    "        #************************\n",
    "        keep_portfolio_before_trade = portfolio_before_trade\n",
    "        #keep_portfolio_before_trade_list.append(keep_portfolio_before_trade)\n",
    "        keep_available_cash_before_trading = self.state[0]\n",
    "\n",
    "        #Extracting the indicies of sell action\n",
    "        #if actions is not an array then convert it to array using np.array()\n",
    "        sell_indices = np.where(actions <  0 )[0]\n",
    "        #Extracting the indicies of buy action\n",
    "        buy_indices = np.where(actions >  0 )[0]\n",
    "\n",
    "\n",
    "        #************************\n",
    "        #Hold indicies\n",
    "        hold_indices = np.where(actions ==  0 )[0]\n",
    "\n",
    "\n",
    "\n",
    "        #print(actions)\n",
    "        #print(self.ticker)\n",
    "\n",
    "\n",
    "        ###### Trading starts #######\n",
    "        print(' ')\n",
    "        print('*************** Trading start ***************')\n",
    "        print(' ')\n",
    "        # Initially selling the stocks to increase investment value\n",
    "        \n",
    "        for idx in sell_indices:\n",
    "\n",
    "          #************************\n",
    "          sell_ticker_symbol = self.ticker[idx]\n",
    "          \n",
    "\n",
    "          stock_to_sell_ticker = 'Stock_to_sell_' + str(sell_ticker_symbol)\n",
    "          self.keep_row_info[stock_to_sell_ticker] = 'Yes'\n",
    "\n",
    "          #Sell stock if price is > 0 and shares held > 0\n",
    "          if self.state[idx+1] > 0 and self.state[idx+self.Action_Space+1] > 0:\n",
    "\n",
    "            #No of shares to sell\n",
    "            shares_sell = min(self.state[idx+self.Action_Space+1], abs(actions[idx]*self.max_shares_per_trade))\n",
    "        \n",
    "            sell_before_holdings = str(self.state[idx+self.Action_Space+1])\n",
    "                  \n",
    "                  \n",
    "            num_of_ticker_shares_available_before_selling = 'num_of_' + str(sell_ticker_symbol) + 'shares_available_before_selling'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_before_selling] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares to sell: ' + str(shares_sell))\n",
    "            num_of_ticker_to_sell = 'num_of_' + str(sell_ticker_symbol) + '_to_sell:'\n",
    "            self.keep_row_info[num_of_ticker_to_sell] = str(shares_sell)\n",
    "\n",
    "            #Updating the available cash after selling the stocks\n",
    "            self.state[0] += self.state[idx+1]*shares_sell*(1-self.commission)\n",
    "\n",
    "            #Updating the available stocks after selling\n",
    "            self.state[idx+self.Action_Space+1] -= shares_sell\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares available after selling: ' + str(self.state[idx+self.Action_Space+1]))\n",
    "            num_of_ticker_shares_available_after_selling = 'num_of_' + str(sell_ticker_symbol) + 'shares_available_after_selling'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_after_selling] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "            sell_after_holdings = str(self.state[idx+self.Action_Space+1])\n",
    "            \n",
    "            if shares_sell > 0:\n",
    "                sellPrice = self.data[self.data['Ticker'] ==  sell_ticker_symbol]['AdjClose']\n",
    "                print('Sold ' + str(shares_sell) + ' shares of ' + str(sell_ticker_symbol) +' at $'+str(round(sellPrice[0],2))+' a share!') \n",
    "\n",
    "            \n",
    "            #stock_info_df = self.df\n",
    "            #wanted_df_1 = stock_info_df[stock_info_df['Date'] == date_string]\n",
    "            #wanted_df_2 = wanted_df_1[wanted_df_1['Ticker'] == sell_ticker_symbol]\n",
    "            #wanted_df_2 = wanted_df_2[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]]\n",
    "\n",
    "\n",
    "            #for w_column in wanted_df_2.columns.tolist():\n",
    "              #print(str(sell_ticker_symbol))\n",
    "              #print(str(w_column))\n",
    "              #print(str(wanted_df_2[w_column].values.tolist()[0]))\n",
    "              #print(str(sell_ticker_symbol) + '_' + str(w_column)+ ' : ' + str(wanted_df_2[w_column].values.tolist()))\n",
    "\n",
    "            #  self.keep_row_info[str(sell_ticker_symbol) + '_' + str(w_column)] = str(wanted_df_2[w_column].values.tolist()[0])\n",
    "              #pass\n",
    "\n",
    "\n",
    "\n",
    "          else:\n",
    "            # print('No Shares to sell')\n",
    "            pass\n",
    "        #print('Buying Stock shares : ')\n",
    "        for idx in buy_indices:\n",
    "\n",
    "          #************************\n",
    "          #print(\"Stock to buy: \" + str(idx))\n",
    "          buy_ticker_symbol = self.ticker[idx]\n",
    "          #print('Ticker: ' + str(buy_ticker_symbol))\n",
    "\n",
    "          stock_to_buy_ticker = 'Stock_to_buy_' + str(buy_ticker_symbol)\n",
    "          self.keep_row_info[stock_to_buy_ticker] = 'Yes'\n",
    "\n",
    "          #Buy stocks if price is > 0\n",
    "          if self.state[idx+1] > 0 and self.state[0] > 0:\n",
    "\n",
    "            #Max number of shares that can be brought with the available cash (available cash / stock price)\n",
    "            max_shares_buy = self.state[0]*(1 - self.commission)//self.state[idx+1]\n",
    "            #max_shares_buy = int(self.state[0]*(1 - self.commission)//self.state[idx+1]//1000)*1000\n",
    "\n",
    "            #No of shares to buy\n",
    "            shares_buy = min(max_shares_buy, actions[idx]*self.max_shares_per_trade)\n",
    "            #shares_buy = ((min(max_shares_buy, actions[idx]*self.max_shares_per_trade))//1000)*1000\n",
    "\n",
    "            buy_before_holdings = str(self.state[idx+self.Action_Space+1])\n",
    "            \n",
    "            num_of_ticker_shares_available_before_buying = 'num_of_' + str(buy_ticker_symbol) + 'shares_available_before_buying'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_before_buying] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares to buy: ' + str(shares_buy))\n",
    "            num_of_ticker_to_buy = 'num_of_' + str(buy_ticker_symbol) + '_to_buy:'\n",
    "            self.keep_row_info[num_of_ticker_to_buy] = str(shares_buy)\n",
    "\n",
    "            #Updating the available cash after buying the stocks\n",
    "            self.state[0] -= self.state[idx+1]*shares_buy*(1 + self.commission)\n",
    "\n",
    "            #Updating the available stocks after buying\n",
    "            self.state[idx+self.Action_Space+1] += shares_buy\n",
    "\n",
    "            #************************\n",
    "            #print('No. of shares available after buying: ' + str(self.state[idx+self.Action_Space+1]))\n",
    "            num_of_ticker_shares_available_after_buying = 'num_of_' + str(buy_ticker_symbol) + 'shares_available_after_buying'\n",
    "            self.keep_row_info[num_of_ticker_shares_available_after_buying] = str(self.state[idx+self.Action_Space+1])\n",
    "            \n",
    "            buy_after_holdings = str(self.state[idx+self.Action_Space+1])\n",
    "            \n",
    "            if shares_buy > 0:\n",
    "                buyPrice = self.data[self.data['Ticker'] ==  buy_ticker_symbol]['AdjClose']\n",
    "                print('Bought ' + str(shares_buy) + ' shares of ' + str(buy_ticker_symbol) + ' at $' + str(round(buyPrice[0],2)) +' a share!')\n",
    "            \n",
    "            #stock_info_df = self.df\n",
    "            #wanted_df_1 = stock_info_df[stock_info_df['Date'] == date_string]\n",
    "            #wanted_df_2 = wanted_df_1[wanted_df_1['Ticker'] == buy_ticker_symbol]\n",
    "            #wanted_df_2 = wanted_df_2[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]]\n",
    "\n",
    "#@@@\n",
    "            #for w_column in wanted_df_2.columns.tolist():\n",
    "              #print(str(sell_ticker_symbol))\n",
    "              #print(str(w_column))\n",
    "              #print(str(wanted_df_2[w_column].values.tolist()[0])\n",
    "              #print(str(sell_ticker_symbol) + '_' + str(w_column)+ ' : ' + str(wanted_df_2[w_column].values.tolist()))\n",
    "\n",
    "            #  self.keep_row_info[str(buy_ticker_symbol) + '_' + str(w_column)] = str(wanted_df_2[w_column].values.tolist()[0])\n",
    "              #pass\n",
    "\n",
    "          else:\n",
    "\n",
    "            #************************\n",
    "            for idx in hold_indices:\n",
    "              #print(\"Stock to hold: \" + str(idx))\n",
    "              hold_ticker_symbol = self.ticker[idx]\n",
    "              #print('Ticker: ' + str(hold_ticker_symbol))\n",
    "\n",
    "              stock_to_hold_ticker = 'Stock_to_hold_' + str(hold_ticker_symbol)\n",
    "              self.keep_row_info[stock_to_hold_ticker] = 'Yes'\n",
    "\n",
    "              num_of_ticker_shares_available_for_holding = 'num_of_' + str(hold_ticker_symbol) + 'shares_available_for_holding'\n",
    "              self.keep_row_info[num_of_ticker_shares_available_for_holding] = str(self.state[idx+self.Action_Space+1])\n",
    "\n",
    "              #stock_info_df = self.df\n",
    "              #wanted_df_1 = stock_info_df[stock_info_df['Date'] == date_string]\n",
    "              #wanted_df_2 = wanted_df_1[wanted_df_1['Ticker'] == hold_ticker_symbol]\n",
    "              #wanted_df_2 = wanted_df_2[['Date', 'Ticker', 'Open', 'High', 'Low', 'AdjClose', 'Volume','momentum_ppo', 'momentum_rsi','trend_adx', 'trend_macd', 'trend_cci' ]]\n",
    "\n",
    "#@@@\n",
    "              #for w_column in wanted_df_2.columns.tolist():\n",
    "                #print(str(sell_ticker_symbol))\n",
    "                #print(str(w_column))\n",
    "                #print(str(wanted_df_2[w_column].values.tolist()[0]))\n",
    "                #print(str(hold_ticker_symbol) + '_' + str(w_column)+ ' : ' + str(wanted_df_2[w_column].values.tolist()))\n",
    "\n",
    "              #  self.keep_row_info[str(hold_ticker_symbol) + '_' + str(w_column)] = str(wanted_df_2[w_column].values.tolist()[0])\n",
    "                #pass\n",
    "\n",
    "            # print('No Shares purchased')\n",
    "            pass\n",
    "        ###### Trading ends #######\n",
    "        print(' ')\n",
    "        print('*************** Trading Ends ***************')\n",
    "        print(' ')\n",
    "        print('** Stocks and holdings after trade: **')\n",
    "        for tidx in range(len(tic)):\n",
    "            ticker_symbol = self.ticker[tidx]\n",
    "            ticker_holdings = str(round(self.state[tidx+self.Action_Space+1]))\n",
    "            print(ticker_symbol + ': ' + ticker_holdings + ' Shares')\n",
    "        \n",
    "        portfolio_after_trade = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "        \n",
    "        print(' ')\n",
    "        print('Available cash for after trading : {}'.format(self.state[0]))\n",
    "        # print('Shares available per stock : '.format(np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1])))\n",
    "        print('Stocks portfolio value after trading : {}'.format(portfolio_after_trade))\n",
    "        print('Total portfolio value after trading: ' + str(portfolio_after_trade + 12000))\n",
    "        #print('Profit or Loss for Day {} : {} is {}'.format(self.day, self.date_memory[-1], self.reward))\n",
    "\n",
    "        #************************\n",
    "        keep_available_cash_after_trading = self.state[0]\n",
    "        keep_shares_available_per_stock = np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1])\n",
    "        keep_profit_and_loss_for_day = self.reward\n",
    "        #keep_available_cash_after_trading_list.append(keep_available_cash_after_trading)\n",
    "        #keep_shares_available_per_stock_list.append(keep_shares_available_per_stock)\n",
    "        #keep_profit_and_loss_for_day_list.append(Keep_profit_and_loss_for_day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Setting the values for next trading date\n",
    "        self.day += 1\n",
    "\n",
    "        #************************\n",
    "\n",
    "\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "\n",
    "        self.state = [self.state[0]] + self.data.AdjClose.values.tolist() + list(self.state[(self.Action_Space+1):(2*self.Action_Space)+1]) + \\\n",
    "                      self.data.momentum_ppo.values.tolist() + self.data.momentum_rsi.values.tolist() + \\\n",
    "                      self.data.trend_adx.values.tolist() + self.data.trend_macd.values.tolist() + \\\n",
    "                      self.data.trend_cci.values.tolist()\n",
    "\n",
    "\n",
    "        portfolio_after_trade = self.state[0] + sum(np.array(self.state[1:self.Action_Space+1])*\n",
    "                                                     np.array(self.state[self.Action_Space+1:(2*self.Action_Space)+1]))\n",
    "\n",
    "        #************************\n",
    "        keep_portfolio_after_trade = portfolio_after_trade\n",
    "        #print('Portfolio value after trading:  ' + str(keep_portfolio_after_trade))\n",
    "        #print('Available Cash before trading:  ' + str(keep_available_cash_before_trading))\n",
    "        #print('Available Cash after trading:   ' + str(keep_available_cash_after_trading))\n",
    "        #print('Profit and loss for day:        ' + str(keep_profit_and_loss_for_day))\n",
    "\n",
    "        self.keep_row_info['portfolio_value_after_trading'] = str(keep_portfolio_after_trade)\n",
    "        self.keep_row_info['available_cash_before_trading'] = str(keep_available_cash_before_trading)\n",
    "        self.keep_row_info['available_cash_after_trading'] = str(keep_available_cash_after_trading)\n",
    "        self.keep_row_info['profit_and_loss_for_day'] = str(keep_profit_and_loss_for_day)\n",
    "\n",
    "        #Total trade in a day (profit or loss)\n",
    "        self.reward = portfolio_after_trade - portfolio_before_trade\n",
    "        #print('Day : {} | Reward : {}'.format(self.day-1, self.reward))\n",
    "        self.reward_memory.append(self.reward)\n",
    "        self.asset_memory.append(portfolio_after_trade)\n",
    "        self.date_memory.append(self.data.Date.unique()[0])\n",
    "        self.reward = self.reward*self.normalized_rewards     #Normalizing the reward\n",
    "\n",
    "        self.keep_rows_list.append(self.keep_row_info)\n",
    "        #print(self.keep_rows_list)\n",
    "\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "    def _seed(self, seed = 10):\n",
    "      randomState, seed = seeding.np_random(seed)\n",
    "      return [seed]\n",
    "\n",
    "    #added for seed\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "def render_trading(model, env, data, n_episodes = 1):\n",
    "  episode_rewards = [0.0]\n",
    "  obs = env.reset()\n",
    "  env.render()\n",
    "\n",
    "  for i in range(n_episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        #print(action)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        # Stats\n",
    "        if done:\n",
    "          obs = env.reset()\n",
    "          #print('Episode {} Rewards {}'.format(i+1, episode_rewards[-1]))\n",
    "          episode_rewards.append(0.0)\n",
    "        else:\n",
    "          #print(rewards)\n",
    "          episode_rewards[-1] += rewards\n",
    "    if (i+1)%100 == 0 and i != 0:\n",
    "      print('Average reward {}'.format(np.average(episode_rewards[:i+1])))\n",
    "\n",
    "  return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f89a72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trading Environment\n",
    "trade_env = DummyVecEnv([lambda : StockTradingEnv2(trading_data, Shares_Per_Trade,\n",
    "                                                  Initial_Investment, Action_Space,\n",
    "                                                  Observation_Space, verbosity=1, mode='trade',\n",
    "                                                  Normalized_Rewards= Normalized_Rewards,\n",
    "                                                  commission = commission)])\n",
    "\n",
    "trade_env = VecNormalize(trade_env, norm_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3741e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VS Buy and Hold function\n",
    "def vs_buy_and_hold(start_date, end_date, holdings_list, starting_cash, other_cash):\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    #### Find start total portfolio ####\n",
    "    starting_stock_dataframe = pd.DataFrame()\n",
    "    original_start_date_object = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    new_start_date_object = original_start_date_object + timedelta(days=1)\n",
    "    day_after_start_date = new_start_date_object.strftime('%Y-%m-%d')\n",
    "\n",
    "    for stock_ticker1 in tic:\n",
    "        temp_data = yfinance.download(stock_ticker1, start=trading_start_date, end=day_after_start_date, progress=False)\n",
    "        temp_data['Ticker'] = stock_ticker1\n",
    "        starting_stock_dataframe = starting_stock_dataframe.append(temp_data)\n",
    "    \n",
    "    starting_stock_dataframe['Shares'] = holdings_list\n",
    "    starting_subtotal_portfolio = sum(starting_stock_dataframe['Shares'] * starting_stock_dataframe['Adj Close'])\n",
    "    ##print(starting_stock_dataframe[['Adj Close','Shares']])\n",
    "    starting_total_portfolio = starting_subtotal_portfolio + starting_cash + other_cash\n",
    "    \n",
    "    #### Find end total portfolio ####\n",
    "    ending_stock_dataframe = pd.DataFrame()\n",
    "    original_end_date_object = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    new_end_date_object = original_end_date_object + timedelta(days=1)\n",
    "    day_after_end_date = new_end_date_object.strftime('%Y-%m-%d')\n",
    "\n",
    "    for stock_ticker2 in tic:\n",
    "        temp_data2 = yfinance.download(stock_ticker2, start=trading_end_date, end=day_after_end_date, progress=False)\n",
    "        temp_data2['Ticker'] = stock_ticker2\n",
    "        ending_stock_dataframe = ending_stock_dataframe.append(temp_data2)\n",
    "        \n",
    "    ending_stock_dataframe['Shares'] = holdings_list\n",
    "    ending_subtotal_portfolio = sum(ending_stock_dataframe['Shares'] * ending_stock_dataframe['Adj Close'])\n",
    "    ##print(ending_stock_dataframe[['Adj Close','Shares']])\n",
    "    ending_total_portfolio = ending_subtotal_portfolio + starting_cash + other_cash\n",
    "    \n",
    "    totalRewards = ending_total_portfolio - starting_total_portfolio\n",
    "    pctOfProfit = (totalRewards*100)/starting_total_portfolio\n",
    "    \n",
    "    return ('Start Date : {} | End Date : {} | Initial Portfolio Value : {} | Final Portfolio Value : {} | Total rewards : {} | % of profit : {}'.format(start_date, end_date, starting_total_portfolio, ending_total_portfolio, totalRewards, pctOfProfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afcc7ad3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Day: 0\n",
      "2024-05-31\n",
      "Available cash before trading: 3768.32\n",
      "CD value: 12000\n",
      "Stocks portfolio value before trading: 53969.76942474365\n",
      "Total portfolio value before trading: 65969.76942474366\n",
      " \n",
      "** Stocks and holdings before trade: **\n",
      "AAPL: 53 Shares\n",
      "AMZN: 10 Shares\n",
      "GOLD: 270 Shares\n",
      "GOOG: 2 Shares\n",
      "GOOGL: 40 Shares\n",
      "META: 10 Shares\n",
      "MSFT: 10 Shares\n",
      "SMCI: 2 Shares\n",
      "SNOW: 20 Shares\n",
      "TSLA: 2 Shares\n",
      "TSM: 70 Shares\n",
      "CRM: 10 Shares\n",
      " \n",
      "** Stocks Technical Indicators: **\n",
      "AAPL\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci  AdjClose\n",
      "0      2.299492     67.294615  33.423601    4.254256  79.732172    192.25\n",
      " \n",
      "AMZN\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd   trend_cci    AdjClose\n",
      "0     -0.392337     36.745153  10.671988   -0.715091 -199.716751  176.440002\n",
      " \n",
      "GOLD\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci  AdjClose\n",
      "0      0.484825     49.885153  16.046388    0.082935 -11.645975     17.09\n",
      " \n",
      "GOOG\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci    AdjClose\n",
      "0      2.030185     55.459818  23.287286    3.484096 -17.537026  173.960007\n",
      " \n",
      "GOOGL\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci  AdjClose\n",
      "0      2.063114     55.674542  22.745251    3.508929 -14.730006     172.5\n",
      " \n",
      "META\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci    AdjClose\n",
      "0     -0.293233     46.509938   13.89113   -1.383955  -86.44658  466.829987\n",
      " \n",
      "MSFT\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci    AdjClose\n",
      "0       0.79438     46.929727  17.309921    3.323865 -65.850911  415.130005\n",
      " \n",
      "SMCI\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci   AdjClose\n",
      "0     -0.773555     41.759772  14.437498   -6.600145 -95.015976  784.51001\n",
      " \n",
      "SNOW\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd   trend_cci    AdjClose\n",
      "0     -2.332818     27.563462  22.950158   -3.620651 -262.273188  136.179993\n",
      " \n",
      "TSLA\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci    AdjClose\n",
      "0      0.899642     52.205518  14.852109     1.58077  11.040327  178.080002\n",
      " \n",
      "TSM\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd  trend_cci    AdjClose\n",
      "0      2.636883     53.286476  29.123809    3.936572  -1.890617  151.039993\n",
      " \n",
      "CRM\n",
      "   momentum_ppo  momentum_rsi  trend_adx  trend_macd   trend_cci    AdjClose\n",
      "0     -3.244339     30.078857  28.027291   -8.820164 -280.621059  234.440002\n",
      " \n",
      " \n",
      "*************** Trading start ***************\n",
      " \n",
      "Sold 10 shares of AMZN at $176.44 a share!\n",
      "Sold 270 shares of GOLD at $17.09 a share!\n",
      "Sold 10 shares of META at $466.83 a share!\n",
      "Sold 2 shares of TSLA at $178.08 a share!\n",
      "Sold 70 shares of TSM at $151.04 a share!\n",
      "Sold 10 shares of CRM at $234.44 a share!\n",
      "Bought 146.0 shares of AAPL at $192.25 a share!\n",
      " \n",
      "*************** Trading Ends ***************\n",
      " \n",
      "** Stocks and holdings after trade: **\n",
      "AAPL: 199 Shares\n",
      "AMZN: 0 Shares\n",
      "GOLD: 0 Shares\n",
      "GOOG: 2 Shares\n",
      "GOOGL: 40 Shares\n",
      "META: 0 Shares\n",
      "MSFT: 10 Shares\n",
      "SMCI: 2 Shares\n",
      "SNOW: 20 Shares\n",
      "TSLA: 0 Shares\n",
      "TSM: 0 Shares\n",
      "CRM: 0 Shares\n",
      " \n",
      "Available cash for after trading : 20.179489440917678\n",
      "Stocks portfolio value after trading : 53969.76942474365\n",
      "Total portfolio value after trading: 65969.76942474366\n",
      " \n",
      "Day: 1\n",
      "2024-06-03\n",
      "Available cash before trading: 20.179489440917678\n",
      " \n",
      "Start Date : 2024-05-31 | End Date : 2024-06-03 | Initial Portfolio Value : 65969.76942474366 | Final Portfolio Value : 65969.76942474366 | Total rewards : 0.0 | % of profit : 0.0\n"
     ]
    }
   ],
   "source": [
    "# Algorithmic Trading result\n",
    "mean_reward, std_reward = evaluate_policy(Best_model, trade_env, n_eval_episodes=1, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8415b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date : 2024-05-31 | End Date : 2024-06-03 | Initial Portfolio Value : 65969.76942474366 | Final Portfolio Value : 65969.76942474366 | Total rewards : 0.0 | % of profit : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(Algorithmic_Trading_Result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5127b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if duplicate_to_next_day == 'yes':\n",
    "    pass\n",
    "else:\n",
    "    trading_start_date = dateRangeList[0]\n",
    "    trading_end_date = dateRangeList[-1]\n",
    "    print(vs_buy_and_hold(trading_start_date, trading_end_date, Current_Tic_Holdings_list, Initial_Investment, cd_cash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09065301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abaeb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
